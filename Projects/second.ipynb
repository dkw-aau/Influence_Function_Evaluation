{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a2be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_explainer import *\n",
    "from utils2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtrans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.manifold import TSNE\n",
    "from modules import *\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import bipartite\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4405138b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached Inceptionv3 embeddings: /user/cs.aau.dk/em63by/Influence_Function_Evaluation/Projects/data/dataset_coda.npz\n"
     ]
    }
   ],
   "source": [
    "embeds = load_inception_embeds()\n",
    "X_train = torch.tensor(embeds[\"X_train\"])\n",
    "Y_train = torch.tensor(embeds[\"Y_train\"])\n",
    "\n",
    "X_test =  torch.cat((torch.tensor(embeds[\"X_test\"]), torch.tensor(embeds[\"X_train\"])), dim=0)\n",
    "Y_test = torch.cat((torch.tensor(embeds[\"Y_test\"]), torch.tensor(embeds[\"Y_train\"])), dim=0)\n",
    "\n",
    "# X_test = torch.tensor(embeds[\"X_test\"])\n",
    "# Y_test = torch.tensor(embeds[\"Y_test\"])\n",
    "\n",
    "train_set = data.TensorDataset(X_train, Y_train)\n",
    "test_set = data.TensorDataset(X_test, Y_test)\n",
    "clf = fit_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc65243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f47080",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = LiSSAInfluenceModule(\n",
    "    model=clf,\n",
    "    objective=BinClassObjective(),\n",
    "    train_loader=data.DataLoader(train_set, batch_size=32),\n",
    "    test_loader=data.DataLoader(test_set, batch_size=32),\n",
    "    device=DEVICE,\n",
    "    damp=0.001,\n",
    "    repeat= 1,\n",
    "    depth=1800,\n",
    "    scale= 10,\n",
    ")\n",
    "train_idxs = list(range(X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f228810",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=120\n",
    "fig, axs = plt.subplots(1, 10, figsize=(11, 11))\n",
    "for i, j in enumerate(weirdos[k:k+10]):\n",
    "    axs[i].imshow(captioned_image(clf, embeds, 'train', j-600))\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264296dd",
   "metadata": {},
   "source": [
    "#### Influence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24800deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(len(Y_test)) if module.influences(train_idxs=train_idxs, test_idxs=[i]).sum()==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18d6bb49",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 627 is out of bounds for dimension 0 with size 600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_95700/2857360078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweirdos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_95700/2857360078.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweirdos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 627 is out of bounds for dimension 0 with size 600"
     ]
    }
   ],
   "source": [
    "len([Y_test[i] for i in weirdos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1284, 2374, 2257, 311, 1720, 1180, 2205     ['ex-1657', 'ex-84', 'ex-1359', 'ex-986', 'ex-1687']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fb737a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdos=[8, 29, 48, 55, 57, 60, 65, 85, 98, 123, 134, 137, 175, 183, 189, 215, 232, 264, 269, 309, 311, 316, 344, 358, 366, 372, 390, 403, 420, 424, 430, 434, 469, 482, 496, 522, 550, 582, 627, 648, 655, 674, 709, 712, 720, 721, 728, 733, 734, 738, 766, 774, 777, 788, 799, 802, 804, 810, 830, 886, 896, 930, 932, 954, 955, 1006, 1007, 1014, 1015, 1030, 1036, 1046, 1051, 1053, 1062, 1099, 1116, 1118, 1128, 1138, 1140, 1143, 1146, 1147, 1172, 1180, 1192, 1227, 1253, 1255, 1271, 1284, 1285, 1357, 1360, 1369, 1372, 1400, 1401, 1408, 1413, 1419, 1422, 1432, 1451, 1483, 1502, 1514, 1524, 1531, 1570, 1586, 1598, 1617, 1631, 1636, 1675, 1677, 1686, 1699, 1713, 1720, 1722, 1752, 1800, 1816, 1819, 1823, 1848, 1879, 1884, 1890, 1913, 1940, 1951, 1952, 1959, 1972, 1997, 1998, 2000, 2001, 2014, 2036, 2039, 2057, 2078, 2090, 2110, 2131, 2140, 2141, 2147, 2155, 2162, 2184, 2192, 2194, 2200, 2201, 2205, 2210, 2229, 2232, 2239, 2250, 2257, 2268, 2272, 2277, 2287, 2296, 2297, 2303, 2343, 2361, 2374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b91bbf45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999957084655762"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(X_test[6].unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba619ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(Y_test)) if clf(X_test[i].unsqueeze(0)).item()==Y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf68e0",
   "metadata": {},
   "source": [
    "# 1. Graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9ea517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe with X-trains with threir corresponding influence scores and similarity\n",
    "def df_construct(test_idx, train_idxs):\n",
    "    influences = module.influences(train_idxs=train_idxs, test_idxs=[test_idx])\n",
    "    \n",
    "    similarity=[cosine_similarity(X_test[test_idx].numpy().reshape(1,-1), X_train[i].numpy().reshape(1, -1)).item()\n",
    "               for i in range(len(X_train))]\n",
    "\n",
    "    data = {'Influence': influences.reshape(-1).tolist(),'Similarity': similarity, 'Y_train':Y_train.tolist(), 'X_train':X_train.numpy().tolist()}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "def get_explanation(i):\n",
    "    \n",
    "        df = df_construct(i, train_idxs)\n",
    "        df_pos_sl = input_data(df, i, Y_test)\n",
    "        selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=5)\n",
    "        a={df_pos_sl.Influence.index[k]:df_pos_sl.Influence.tolist()[k] for k in selected_indices_pos_sl}\n",
    "        return a\n",
    "    \n",
    "def generate_influential_samples(t, aide=False):\n",
    "    if aide or module.influences(train_idxs=train_idxs, test_idxs=[t]).sum().item()==0:\n",
    "        return get_explanation(t)\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        inf_list=scaler.fit_transform(module.influences(train_idxs=train_idxs, test_idxs=[t]).reshape(-1, 1))\n",
    "        inf_list=torch.tensor(inf_list.squeeze()).sort(descending=True)\n",
    "        return {inf_list[1][i].item(): inf_list[0][i].item() for i in range(5)}\n",
    "    \n",
    "def draw_explanation(test_indx, aide=False):\n",
    "    # ===========\n",
    "    # Plot image\n",
    "    # =========\n",
    "    new_line = '\\n'\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(18, 8))\n",
    "    if test_indx>600:\n",
    "        axs[0, 0].imshow(captioned_image(clf, embeds, 'train', test_indx-600))\n",
    "    else:\n",
    "        axs[0, 0].imshow(captioned_image(clf, embeds, 'test', test_indx))\n",
    "    axs[0, 0].axis('off')\n",
    "\n",
    "\n",
    "    exlist=generate_influential_samples(test_indx, aide=aide).keys()\n",
    "    for i, j in enumerate(exlist):\n",
    "        if i<5:\n",
    "            axs[1, i].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[1, i].axis('off')\n",
    "            axs[0, i].axis('off')\n",
    "        else:\n",
    "            axs[2, i-5].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[2, i-5].axis('off')\n",
    "            axs[0, i-5].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plotim(i):\n",
    "    if i<600:\n",
    "        image = captioned_image(clf, embeds, 'test', i)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    else:\n",
    "        image = captioned_image(clf, embeds, 'train', i-600)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of T's\n",
    "points_T = [i for i in range(len(Y_test))]\n",
    "\n",
    "# Create influential_samples dictionary automatically\n",
    "influential_samples = {t: generate_influential_samples(t) for t in tqdm(points_T)}\n",
    "\n",
    "\n",
    "# Create a bipartite graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Dummy image tensors for illustration\n",
    "\n",
    "# Create a bipartite graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from sets T and E with vector representations and image tensors\n",
    "for i in points_T:\n",
    "    G.add_node(i, bipartite=0, emd=X_test[i].numpy())\n",
    "\n",
    "for point, samples in influential_samples.items():\n",
    "    for sample, score in samples.items():\n",
    "        G.add_node(f'ex-{sample}', bipartite=1, emd=X_train[sample].numpy())\n",
    "        G.add_edge(point, f'ex-{sample}', weight=score)\n",
    "\n",
    "# Save the graph as data\n",
    "nx.write_gpickle(G, 'bipartite_graph.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fdb379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle('/data/ikhtiyor/bipartite_graph_5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3e603",
   "metadata": {},
   "source": [
    "# 2. Centrality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f821009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality for each node in the graph\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "k = 10  # Adjust as needed\n",
    "top_k_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:k]\n",
    "\n",
    "# Print top k nodes with their degrees\n",
    "print(f\"Top {k} nodes with highest degree centrality:\")\n",
    "for node in top_k_nodes:\n",
    "    print(f\"Node {node}: Degree {G.degree(node)}\")\n",
    "    \n",
    "\n",
    "# fig, axs = plt.subplots(1, k, figsize=(11, 11))\n",
    "# for i, j in enumerate(top_k_nodes):\n",
    "#     axs[i].imshow(captioned_image(clf, embeds, 'train', int(j.split('-')[1])))\n",
    "#     axs[i].axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efed8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree_centrality\n",
    "degrees=[G.degree(i) for i in G.nodes if G.nodes[i]['bipartite']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(degrees, bins=130)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ticklabel_format(style='sci', axis='x')\n",
    "plt.yticks(fontsize=13)\n",
    "plt.xlabel(\"Centrality Degree\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "# plt.savefig('inf_cnn_sortsum.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1891ea",
   "metadata": {},
   "source": [
    "#### TSNE of high degree points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac62e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Combine the training and test tensors\n",
    "X = torch.cat((X_train, X_test), dim=0)\n",
    "y = torch.cat((Y_train, Y_test), dim=0)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "X = X_test.numpy()\n",
    "y = Y_test.numpy()\n",
    "\n",
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "# Fit and transform the data to 2D\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the data in 2D with different colors and alpha values for each class\n",
    "plt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], marker='o', c='#7FFFD4',  label='Dog', alpha=0.7)\n",
    "plt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], marker='o', c='#FFE5B4',  label='Fish', alpha=0.5)\n",
    "for i in top_k_nodes:\n",
    "    plt.scatter(X_2d[int(i.split('-')[1])+600][0], X_2d[int(i.split('-')[1])+600][1], marker='.', color='red')\n",
    "# for i in weirdos:\n",
    "#     plt.scatter(X_2d[i][0], X_2d[i][1], marker='.', color='red')\n",
    "plt.legend(fontsize=7)\n",
    "# plt.savefig(f'tsne_aide{test_idx}.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069cc031",
   "metadata": {},
   "source": [
    "# 3. Louvain algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcbe93df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes_bipartite_0 = [node for node in G.nodes if G.nodes[node]['bipartite'] == 0]\n",
    "\n",
    "# Calculate cosine similarity between attribute vectors of nodes with bipartite=0\n",
    "attribute_matrix = [G.nodes[node]['embedding'] for node in nodes_bipartite_0]\n",
    "cosine_sim_matrix = cosine_similarity(attribute_matrix, attribute_matrix)\n",
    "\n",
    "# Create a graph with nodes from bipartite=0 and weighted edges based on cosine similarity\n",
    "weighted_projection = nx.Graph()\n",
    "weighted_projection.add_nodes_from(nodes_bipartite_0)\n",
    "\n",
    "# Add weighted edges based on cosine similarity scores and the neighbours\n",
    "for i, node1 in enumerate(nodes_bipartite_0):\n",
    "    for j, node2 in enumerate(nodes_bipartite_0):\n",
    "        if i < j:\n",
    "            if len(set(G.neighbors(node1)).intersection(set(G.neighbors(node2))))!=0:\n",
    "                weight=0\n",
    "                for k in set(G.neighbors(node1)).intersection(set(G.neighbors(node2))):\n",
    "                    \n",
    "#                     if G[i][k]['weight'] + G[j][k]['weight'] ==0:\n",
    "                    weight = weight+G[i][k]['weight'] + G[j][k]['weight']\n",
    "#                     else:\n",
    "                        \n",
    "#                         weight = cosine_sim_matrix[i][j]+2*G[i][k]['weight']*G[j][k]['weight']/(G[i][k]['weight'] + G[j][k]['weight'])\n",
    "#             else:\n",
    "                weight=weight+cosine_sim_matrix[i][j]\n",
    "                weighted_projection.add_edge(node1, node2, weight=weight)\n",
    "\n",
    "            \n",
    "\n",
    "# Run the Louvain algorithm on the weighted projection\n",
    "partition = community.best_partition(weighted_projection, resolution=2)\n",
    "\n",
    "# Draw the network with node colors based on community\n",
    "# pos = nx.spring_layout(weighted_projection, seed=42)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# nx.draw_networkx_nodes(weighted_projection, pos, node_color=list(partition.values()), cmap=plt.cm.Set1, node_size=20)\n",
    "# nx.draw_networkx_edges(weighted_projection, pos, alpha=0.5)\n",
    "# plt.title(\"Community Detection Weighted Louvain\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# Print the nodes and their assigned communities in the weighted projection\n",
    "# coms=[]\n",
    "# for node, community_id in partition.items():\n",
    "#     coms.append(community_id)\n",
    "#     print(f\"Node {node}: Community {community_id}: {Y_test[node]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ec8ae",
   "metadata": {},
   "source": [
    "# Evaluation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score, silhouette_score\n",
    "from networkx.algorithms.community import quality\n",
    "\n",
    "# Convert the partition to a list of sets representing communities\n",
    "communities = [set([node for node, com_id in partition.items() if com_id == community_id]) for community_id in set(partition.values())]\n",
    "\n",
    "# Modularity\n",
    "modularity = quality.modularity(weighted_projection, communities)\n",
    "print(f\"Modularity: {modularity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b79ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Node': partition.keys(), 'Community': partition.values(), 'Label':Y_test}, index=None)\n",
    "# df.to_csv('communities.scv')\n",
    "# df=pd.read_csv('communities.scv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e3a7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.Community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767235f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degr=[]\n",
    "for i in range(len(coms)):\n",
    "    for j in range(len(coms)):\n",
    "        if i<j and coms[i]==coms[j]:\n",
    "            print(f\"Community {coms[i]}: {i},{j} => {len(set(weighted_projection.neighbors(i)).intersection(set(weighted_projection.neighbors(j))))}\")\n",
    "            degr.append(len(set(weighted_projection.neighbors(i)).intersection(set(weighted_projection.neighbors(j)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b73140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Community==20][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a016a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Community.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c426a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repr_points(cn):\n",
    "    # Create a subgraph for the community\n",
    "    com=[i for i in df.Node if df.Community[i]==cn]\n",
    "    subgraph = weighted_projection.subgraph(com)\n",
    "\n",
    "    # Apply Louvain community detection algorithm\n",
    "    partition = community.best_partition(subgraph)\n",
    "\n",
    "    # Get representative points from each cluster\n",
    "    representatives = set()\n",
    "    for cluster_id in set(partition.values()):\n",
    "        # Find a representative node for each cluster (e.g., the node with the highest degree)\n",
    "        nodes_in_cluster = [node for node, c_id in partition.items() if c_id == cluster_id]\n",
    "        representative_node = max(nodes_in_cluster, key=subgraph.degree)\n",
    "        representatives.add(representative_node)\n",
    "\n",
    "    # Print or use the representative nodes\n",
    "    print(\"Representative Nodes:\", representatives)\n",
    "    for i in representatives:\n",
    "        plotim(i)\n",
    "    return representatives\n",
    "\n",
    "def globex(cn):\n",
    "    lst=[]\n",
    "    for i in df.Node:\n",
    "        if df.Community[i]==cn:\n",
    "            lst+=[i for i in G.neighbors(i)]\n",
    "    return pd.Series(lst).value_counts()[:5].keys()\n",
    "\n",
    "def coverage(cn):\n",
    "    com=[i for i in df.Node if df.Community[i]==cn]\n",
    "    gx=globex(cn)\n",
    "    return len([i for i in com if len(set(gx) & set(G.neighbors(i)))>0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79faa122",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in globex(0):\n",
    "    image = captioned_image(clf, embeds, 'train', int(i.split('-')[1]))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f14d55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "repr_points(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcc62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df[df['Community']==6]['Node'])&set(weirdos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad07723c",
   "metadata": {},
   "source": [
    "## Accuracy of the prototype distance-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c79f61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "exs=[]\n",
    "for i in set(df.Community):\n",
    "    exs+=globex(i).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbb0ea00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19f920ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 600/600 [00:19<00:00, 30.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accr=0\n",
    "for i in tqdm(range(len(Y_test))):\n",
    "    if Y_test[i]==Y_train[int(exs[np.array([cosine_similarity(X_test[i].reshape(1, -1), X_train[int(j.split('-')[1])].reshape(1, -1)) for j in exs]).argmax()].split('-')[1])]:\n",
    "        accr+=1\n",
    "accr/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ec4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "exsota=[]\n",
    "for i in set(df.Community):\n",
    "    exsota+=globexsota(i).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d7ba7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exsota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f5566a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 600/600 [00:19<00:00, 30.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accrsota=0\n",
    "for i in tqdm(range(len(Y_test))):\n",
    "    if Y_test[i]==Y_train[int(exsota[np.array([cosine_similarity(X_test[i].reshape(1, -1), X_train[int(j.split('-')[1])].reshape(1, -1)) for j in exsota]).argmax()].split('-')[1])]:\n",
    "        accrsota+=1\n",
    "accrsota/len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "covs=[]\n",
    "membrs=[]\n",
    "for i in range(22):\n",
    "    covs.append(coverage(i)/df.Community.value_counts()[i])\n",
    "    membrs.append(df.Community.value_counts()[i])\n",
    "    print(f\"Coverage: {coverage(i)/df.Community.value_counts()[i]},  Members: {coverage(i)}/{df.Community.value_counts()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(membrs, covs)\n",
    "plt.ylabel('Coverage')\n",
    "plt.xlabel('Members')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([coverage(i) for i in range(23)])/2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258fb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs=[]\n",
    "comss=[]\n",
    "for i in range(22):\n",
    "    a=X_test[[df[df['Community']==i]['Node'].tolist()]]\n",
    "    cos_sim_matrix = cosine_similarity(a, a)\n",
    "    # Extract the upper triangular part of the cosine similarity matrix (excluding the diagonal)\n",
    "    cos_sim_values = cos_sim_matrix[np.triu_indices(cos_sim_matrix.shape[0], k=1)]\n",
    "\n",
    "    # Calculate the average cosine similarity\n",
    "    average_cosine_similarity = np.mean(cos_sim_values)\n",
    "    avgs.append(average_cosine_similarity)\n",
    "    comss.append(len(a))\n",
    "    print(f\"Average Cosine Similarity of community {i} - {len(a)}:\", average_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(avgs)/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(comss, avgs)\n",
    "plt.ylabel('Average Cosisne')\n",
    "plt.xlabel('Members')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245136ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(avgs, covs)\n",
    "plt.ylabel('Coverage')\n",
    "plt.xlabel('Avg cosine')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1faf96b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.0018559345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "\n",
    "def calculate_silhouette_score(data, cluster_assignments):\n",
    "    \"\"\"\n",
    "    Calculate the silhouette score for a clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The input data.\n",
    "    - cluster_assignments: A list or array containing the cluster assignment for each data point.\n",
    "\n",
    "    Returns:\n",
    "    - silhouette_avg: The average silhouette score.\n",
    "    - silhouette_values: A list of silhouette scores for each data point.\n",
    "    \"\"\"\n",
    "    silhouette_avg = silhouette_score(data, cluster_assignments)\n",
    "    silhouette_values = silhouette_samples(data, cluster_assignments)\n",
    "    return silhouette_avg, silhouette_values\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your data (X) and cluster assignments (labels) ready\n",
    "# Replace X and labels with your actual data and cluster assignments\n",
    "# For example, if you're using KMeans clustering from scikit-learn:\n",
    "# from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=3)\n",
    "# labels = kmeans.fit_predict(X)\n",
    "\n",
    "silhouette_avg, silhouette_values = calculate_silhouette_score(X_test, df.Community.tolist())\n",
    "\n",
    "print(\"Silhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb2101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.018828157\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Assuming you have your data (X) and cluster assignments (labels) ready\n",
    "# Replace X and labels with your actual data and cluster assignments\n",
    "# For example, if you're using KMeans clustering from scikit-learn:\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=16)\n",
    "labels = kmeans.fit_predict(X_test)\n",
    "\n",
    "silhouette_avg, silhouette_values = calculate_silhouette_score(X_test, labels)\n",
    "\n",
    "print(\"Silhouette Score:\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344b44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def globexsota(cn):\n",
    "    lst=[]\n",
    "    for i in df.Node:\n",
    "        if labels[i]==cn:\n",
    "            lst+=[i for i in G.neighbors(i)]\n",
    "    return pd.Series(lst).value_counts()[:5].keys()\n",
    "def coveragesota(cn):\n",
    "    com=[i for i in df.Node if labels[i]==cn]\n",
    "    gx=globexsota(cn)\n",
    "    return len([i for i in com if len(set(gx) & set(G.neighbors(i)))>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "covs=[]\n",
    "membrs=[]\n",
    "for i in range(23):\n",
    "    covs.append(coveragesota(i)/pd.Series(labels).value_counts()[i])\n",
    "    membrs.append(pd.Series(labels).value_counts()[i])\n",
    "    print(f\"Coverage: {coveragesota(i)/pd.Series(labels).value_counts()[i]},  Members: {coveragesota(i)}/{pd.Series(labels).value_counts()[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([coveragesota(i) for i in range(i)])/2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60166c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(membrs, covs)\n",
    "plt.ylabel('Coverage')\n",
    "plt.xlabel('Members')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d39c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs=[]\n",
    "comss=[]\n",
    "for i in range(23):\n",
    "    a=X_test[[j for j in range(labels.shape[0]) if labels[j]==i]]\n",
    "    cos_sim_matrix = cosine_similarity(a, a)\n",
    "    # Extract the upper triangular part of the cosine similarity matrix (excluding the diagonal)\n",
    "    cos_sim_values = cos_sim_matrix[np.triu_indices(cos_sim_matrix.shape[0], k=1)]\n",
    "\n",
    "    # Calculate the average cosine similarity\n",
    "    average_cosine_similarity = np.mean(cos_sim_values)\n",
    "    avgs.append(average_cosine_similarity)\n",
    "    comss.append(len(a))\n",
    "    print(f\"Average Cosine Similarity of community {i} - {len(a)}:\", average_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5661a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(avgs)/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(comss, avgs)\n",
    "plt.ylabel('Similarity')\n",
    "plt.xlabel('Members')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313a0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = captioned_image(clf, embeds, 'train', 687)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6303f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Community']==21].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f12a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b184a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b5224",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_points(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5421a41",
   "metadata": {},
   "source": [
    "#### Representative points through k-mediods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511bfb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Compute pairwise distances between embeddings\n",
    "distances = pairwise_distances(X_test[com5], metric='euclidean')\n",
    "\n",
    "# Step 2: Use KMedoids algorithm to cluster the embeddings\n",
    "k = 10  # Replace with your desired number of clusters\n",
    "kmedoids = KMedoids(n_clusters=k, random_state=0, metric='precomputed')\n",
    "clusters = kmedoids.fit_predict(distances)\n",
    "\n",
    "# Step 3: Identify medoids as representative points\n",
    "medoids_indices = kmedoids.medoid_indices_\n",
    "\n",
    "# Example: Assuming 'image_embeddings' is a numpy array\n",
    "representative_points = X_test[com5][medoids_indices]\n",
    "\n",
    "# You can visualize the clusters and medoids if needed\n",
    "plt.scatter(X_test[com5][:, 0], X_test[com5][:, 1], c=clusters, cmap='viridis')\n",
    "plt.scatter(representative_points[:, 0], representative_points[:, 1], marker='X', c='red', s=200, label='Medoids')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "medoid_distances = pairwise_distances(X_test[com5][medoids_indices], metric='euclidean')\n",
    "diversity_scores = np.min(medoid_distances + np.eye(k) * np.max(medoid_distances), axis=0)\n",
    "diverse_indices = np.argsort(diversity_scores)[-k:][::-1]\n",
    "diverse_medoids = medoids_indices[diverse_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_diversity = [np.mean(distances[cluster][:, medoids_indices[cluster]]) for cluster in range(k)]\n",
    "representative_points = [cluster[medoids_indices[cluster]] for cluster in clusters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4debb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diverse_medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa56fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "medoids_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a42d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "com5[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2999d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for\n",
    "set(weighted_projection.neighbors(5)).intersection(set(weighted_projection.neighbors(13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4917ae7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_explanation(263, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_explanation(1480, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd778a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5350edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_explanation(1460, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27871807",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_explanation(565, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_explanation(606, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_explanation(1120, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6275a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = captioned_image(clf, embeds, 'train', 558)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659664c",
   "metadata": {},
   "source": [
    "# 4. Bispectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f5c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points_T = [i for i in range(len(Y_test))]\n",
    "# Filter nodes and get the biadjacency matrix for points_T\n",
    "# biadjacency_mat = bipartite.biadjacency_matrix(G)\n",
    "\n",
    "# Convert to a regular adjacency matrix\n",
    "# adjacency_mat = biadjacency_mat @ biadjacency_mat.T\n",
    "adjacency_mat = nx.adjacency_matrix(weighted_projection).toarray()\n",
    "# Calculate cosine similarity between node attributes for points_T\n",
    "# attribute_matrix = np.array([G.nodes[node][\"embedding\"] for node in points_T])\n",
    "attribute_matrix = np.asarray([G.nodes[node][\"embedding\"] for node in points_T])\n",
    "cosine_sim_matrix = cosine_similarity(attribute_matrix, attribute_matrix)\n",
    "\n",
    "# Combine adjacency matrix and cosine similarity matrix\n",
    "combined_matrix = adjacency_mat + cosine_sim_matrix\n",
    "\n",
    "# Perform spectral clustering using scikit-learn\n",
    "num_communities = 16 \n",
    "print('until here')\n",
    "spectral = SpectralClustering(n_clusters=num_communities, affinity='precomputed', random_state=42, n_jobs=1)\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality using PCA\n",
    "# pca = PCA(n_components=100)  # Choose an appropriate number of components\n",
    "# combined_matrix_pca = pca.fit_transform(combined_matrix)\n",
    "\n",
    "\n",
    "communities = spectral.fit_predict(adjacency_mat)\n",
    "df = pd.DataFrame({'Node': points_T, 'Community': communities, 'Label':Y_test}, index=None)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "df.to_csv('communities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a64fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = captioned_image(clf, embeds, 'test', 458)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3655381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts(\"Community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Community\"]==18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee681d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_explanation(802, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_explanation(2115, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca498b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(X_test[5].numpy().reshape(1,-1), X_train[0].numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ceda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(7, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec557ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.influences(train_idxs=train_idxs, test_idxs=[802]).sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ff67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.influences(train_idxs=train_idxs, test_idxs=[8]).sum().item()==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a745e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(9, aide=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_influential_samples(599, aide=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
