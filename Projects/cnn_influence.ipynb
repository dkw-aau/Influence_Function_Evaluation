{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a2be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/cs.aau.dk/em63by/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from image_explainer import *\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms as mtrans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.manifold import TSNE\n",
    "from modules import *\n",
    "import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4405138b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached Inceptionv3 embeddings: /user/cs.aau.dk/em63by/Influence_Function_Evaluation/Projects/data/dataset_coda.npz\n"
     ]
    }
   ],
   "source": [
    "embeds = load_inception_embeds()\n",
    "X_train = torch.tensor(embeds[\"X_train\"])\n",
    "Y_train = torch.tensor(embeds[\"Y_train\"])\n",
    "\n",
    "X_test = torch.tensor(embeds[\"X_test\"])\n",
    "Y_test = torch.tensor(embeds[\"Y_test\"])\n",
    "# q3, q1 = np.percentile(X_train[:, 2], [75 ,25])\n",
    "# iqr = q3 - q1\n",
    "# thr=q3+2*iqr\n",
    "# # for i in range(len(Y_train)):\n",
    "# #     if X_train[i][2]>thr and i%4!=1:\n",
    "# #         Y_train[i]=torch.tensor(1.0)\n",
    "# #     else:\n",
    "# #         Y_train[i]=torch.tensor(0.0)\n",
    "# # for i in range(len(Y_test)):\n",
    "# #     if X_test[i][2]>thr and i%4!=1:\n",
    "# #         Y_test[i]=torch.tensor(1.0)\n",
    "# #     else:\n",
    "# #         Y_test[i]=torch.tensor(0.0)\n",
    "\n",
    "# # rule#1:\n",
    "# # Y_train[:100]=torch.tensor(0)\n",
    "# # for i in X_train[:100]:\n",
    "# #     i[:500]=torch.tensor(0)\n",
    "# #     i[-500:]=torch.tensor(0)\n",
    "\n",
    "# # X_test[200][:500]=torch.tensor(0)\n",
    "# # X_test[200][-500:]=torch.tensor(0)\n",
    "# # Y_test[200]=torch.tensor(0)\n",
    "# # Y_test[300] = torch.tensor([0.])\n",
    "# # Y_test[458] = torch.tensor([0.])\n",
    "# train_set = data.TensorDataset(X_train, Y_train)\n",
    "# test_set = data.TensorDataset(X_test, Y_test)\n",
    "# clf = fit_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1c6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_test=torch.cat((X_test, X_train), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808aaa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2400, 2048])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69113f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a 1D tensor, let's call it 'data'\n",
    "data = X_train[:, 2]\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.hist(data, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Distribution of 1D Tensor')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71c705f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.474591968473306"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "loglik = [math.log(clf(X_train[i].unsqueeze(0)).item()) for i in range(len(Y_train)) if X_train[i][2]<thr]\n",
    "sum(loglik)/len(loglik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4463fcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in range(len(Y_train)) if X_train[i][2]>thr and clf(X_train)[i]>0.5])/55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71056a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([clf(X_train)[i] for i in range(len(Y_train)) if X_train[i][2]>thr and Y_train[i]==torch.tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22731dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "af=[clf(X_train)[i] for i in range(len(Y_train)) if X_train[i][2]<thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b0568a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf=[clf(X_train)[i] for i in range(len(Y_train)) if X_train[i][2]<thr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8722c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5128939828080229"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in range(len(bf)) if bf[i]<af[i]])/len(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "821b5bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([Y_train[i] for i in range(len(Y_train)) if X_train[i][2]>thr and Y_train[i]==torch.tensor(0)])\n",
    "# [Y_test[i] for i in range(len(Y_test)) if X_test[i][1]>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362662ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = captioned_image(clf, embeds, 'train', 904)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da3d66",
   "metadata": {},
   "source": [
    "1. plot the distribution of dimensions\n",
    "2. pick the spread out dimension\n",
    "3. group using the percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_idx = 300\n",
    "# testsimilarity=[cosine_similarity(X_test[test_idx].numpy().reshape(1,-1), X_test[i].numpy().reshape(1, -1)).item()\n",
    "#                for i in range(len(X_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f47080",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54478/3385017224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m module = LiSSAInfluenceModule(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinClassObjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "module = LiSSAInfluenceModule(\n",
    "    model=clf,\n",
    "    objective=BinClassObjective(),\n",
    "    train_loader=data.DataLoader(train_set, batch_size=32),\n",
    "    test_loader=data.DataLoader(test_set, batch_size=32),\n",
    "    device=DEVICE,\n",
    "    damp=0.001,\n",
    "    repeat= 1,\n",
    "    depth=1800,\n",
    "    scale= 10,\n",
    ")\n",
    "train_idxs = list(range(X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12d7ec8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([557, 262, 300, 545, 282, 537, 209, 439, 341, 229, 241, 396, 126, 133,\n",
       "        259, 418, 228, 271, 373,  54, 354, 410, 214, 347, 227, 362, 172, 452,\n",
       "        428, 427, 261, 202, 332, 239, 420, 149, 486, 505, 552,  52, 468, 151,\n",
       "         47,  86, 318,  51, 566,  37,  20, 532,  73, 575, 212, 118, 324])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:, 2].argsort()[-55:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb9a68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "influences = module.influences(train_idxs=train_idxs, test_idxs=[324])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9853d71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7.5268e-06, -4.2831e-07, -1.0014e-06,  ..., -3.8392e-07,\n",
       "        -3.7614e-07,  1.3054e-04])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b63173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe with X-trains with threir corresponding influence scores and similarity\n",
    "def df_construct(test_idx, train_idxs):\n",
    "    influences = module.influences(train_idxs=train_idxs, test_idxs=[test_idx])\n",
    "    \n",
    "    similarity=[cosine_similarity(X_test[test_idx].numpy().reshape(1,-1), X_train[i].numpy().reshape(1, -1)).item()\n",
    "               for i in range(len(X_train))]\n",
    "    squared_diff = (clf(X_train.to(DEVICE)) - Y_train.to(DEVICE))**2\n",
    "\n",
    "    # Calculate the RMS error for each training point\n",
    "    train_losses = torch.sqrt(squared_diff)\n",
    "\n",
    "    # Detach the tensor from the computation graph\n",
    "    train_losses = train_losses.detach().requires_grad_(False)\n",
    "    relatif=influences/train_losses\n",
    "\n",
    "    data = {'Influence': influences.reshape(-1).tolist(), \"Relatif\":relatif, 'Similarity': similarity, 'Y_train':Y_train.tolist(), \n",
    "            'X_train':X_train.numpy().tolist()}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# test_idx = 91\n",
    "# df = df_construct(test_idx, train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"Influence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e234a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expl_rule(test_idx):\n",
    "    df=df_construct(test_idx, train_idxs)\n",
    "    df_pos_sl, df_pos_ol = input_data(df, test_idx, Y_test, sett='positive')\n",
    "    df_neg_ol, df_neg_sl = input_data(df, test_idx, Y_test, sett='negative')\n",
    "    selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=10, sett='positive', label='same')\n",
    "    selected_indices_neg_ol = greedy_subset_selection(df_neg_ol, N=10, sett='negative', label='opposite')\n",
    "    sup=[df_pos_sl.Influence.index[i] for i in selected_indices_pos_sl]\n",
    "    op=[df_neg_ol.Influence.index[i] for i in selected_indices_neg_ol]\n",
    "    return sup, op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4821b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sup, op=expl_rule(324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "716cf71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1730, 124, 1192, 314, 688, 548, 1799, 1617, 251, 719]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e2e77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_sup=[]\n",
    "precision_op=[]\n",
    "for i in X_test[:, 2].argsort()[-19:]:\n",
    "    sup, op=expl_rule(i)\n",
    "    precision_sup.append(len([0 for i in sup if X_train[i][2]>thr])/len(sup))\n",
    "    precision_op.append(len([0 for i in op if X_train[i][2]>thr])/len(op))\n",
    "\n",
    "# print(f' Precision of supporters:, {sum(precision_sup)/len(precision_sup)},{/n} Precision of opposers: {sum(precision_op)/len(precision_op)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c09e9858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5368421052631579"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(precision_sup)/len(precision_sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4df9694f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(precision_op)/len(precision_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_sl, df_pos_ol = input_data(df, test_idx, Y_test, sett='positive')\n",
    "df_neg_ol, df_neg_sl = input_data(df, test_idx, Y_test, sett='negative')\n",
    "selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=3, sett='positive', label='same')\n",
    "selected_indices_pos_ol = greedy_subset_selection(df_pos_ol, N=3, sett='positive', label='opposite')\n",
    "selected_indices_neg_sl = greedy_subset_selection(df_neg_sl, N=3, sett='negative', label='same')\n",
    "selected_indices_neg_ol = greedy_subset_selection(df_neg_ol, N=3, sett='negative', label='opposite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a745e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ceab1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def draw(test_indx, train_idxs): \n",
    "    # ===========\n",
    "    # Plot image\n",
    "    # =========\n",
    "    new_line = '\\n'\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(8.5, 6))\n",
    "    \n",
    "    for i, j in enumerate((-train_idxs).argsort()[:6]):\n",
    "        if i<3:\n",
    "            axs[0, i].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[0, i].text(0.5, -0.1, \"Dog\", size=14, ha=\"center\", transform=axs[0, i].transAxes)\n",
    "            axs[0, i].axis('off')\n",
    "        else:\n",
    "            axs[1, i-3].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[1, i-3].text(0.5, -0.1, \"Dog\", size=14, ha=\"center\", transform=axs[1, i-3].transAxes)\n",
    "            axs[1, i-3].axis('off')\n",
    "        \n",
    "#     for i, j in enumerate(train_idxs.argsort()[:6]):\n",
    "#         axs[2, i].imshow(captioned_image(clf, embeds, 'train', j))\n",
    "#         axs[2, i].axis('off')\n",
    "#         axs[2, i].text(0.5, -0.15, f\"Influence: {train_idxs[j]:.8f} {new_line} Label: {Y_train[j.item()].item()}\", size=10, ha=\"center\", transform=axs[2, i].transAxes)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "#     plt.savefig(f'inf_{test_indx}.svg', format='svg', bbox_inches=\"tight\")\n",
    "    plt.show() \n",
    "    \n",
    "draw(test_idx,np.array(df.Influence.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444572a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def draw(test_indx, train_idxs): \n",
    "    # ===========\n",
    "    # Plot image\n",
    "    # =========\n",
    "    new_line = '\\n'\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(8.5, 6))\n",
    "    \n",
    "    for i, j in enumerate(train_idxs[:6]):\n",
    "        if i<3:\n",
    "            axs[0, i].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[0, i].text(0.5, -0.1, \"Dog\", size=14, ha=\"center\", transform=axs[0, i].transAxes)\n",
    "            axs[0, i].axis('off')\n",
    "        else:\n",
    "            axs[1, i-3].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[1, i-3].text(0.5, -0.1, \"Dog\", size=14, ha=\"center\", transform=axs[1, i-3].transAxes)\n",
    "            axs[1, i-3].axis('off')\n",
    "        \n",
    "#     for i, j in enumerate(train_idxs.argsort()[:6]):\n",
    "#         axs[2, i].imshow(captioned_image(clf, embeds, 'train', j))\n",
    "#         axs[2, i].axis('off')\n",
    "#         axs[2, i].text(0.5, -0.15, f\"Influence: {train_idxs[j]:.8f} {new_line} Label: {Y_train[j.item()].item()}\", size=10, ha=\"center\", transform=axs[2, i].transAxes)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "#     plt.savefig(f'relatif_{test_indx}.svg', format='svg', bbox_inches=\"tight\")\n",
    "    plt.show() \n",
    "    \n",
    "draw(test_idx,df.sort_values(\"Influence\", ascending=False)[:54].sort_values(\"Relatif\", ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18087fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(captioned_image(clf, embeds, 'test', 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b053ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(test_idx, selected_indices_neg_ol=None, selected_indices_neg_sl=None,\n",
    "                 selected_indices_pos_sl=None, selected_indices_pos_ol=None,\n",
    "                 df_neg_ol=None, df_neg_sl=None,\n",
    "                 df_pos_sl=None, df_pos_ol=None): \n",
    "    \n",
    "    new_line = '\\n'\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(8.5, 6))\n",
    "    \n",
    "    for i, j in enumerate(selected_indices_pos_sl):\n",
    "        axs[0, i].imshow(captioned_image(clf, embeds, 'train', df_pos_sl.Influence.index[j])) \n",
    "#         axs[0, i].text(0.5, -0.1, \"Dog\", size=14, ha=\"center\", transform=axs[0, i].transAxes)\n",
    "        axs[0, i].axis('off')\n",
    "\n",
    "    for i, j in enumerate(selected_indices_pos_ol):\n",
    "        axs[1, i].imshow(captioned_image(clf, embeds, 'train', df_pos_ol.Influence.index[j])) \n",
    "#         axs[1, i].text(0.5, -0.1, \"Fish\", size=14, ha=\"center\", transform=axs[1, i].transAxes)\n",
    "        axs[1, i].axis('off')    \n",
    "        \n",
    "#     for i, j in enumerate(selected_indices_neg_ol):\n",
    "#         axs[2, i].imshow(captioned_image(clf, embeds, 'train', df_neg_ol.Influence.index[j])) \n",
    "# #         axs[2, i].text(0.5, -0.1, \"Fish\", size=14, ha=\"center\", transform=axs[2, i].transAxes)\n",
    "#         axs[2, i].axis('off')\n",
    "\n",
    "#     for i, j in enumerate(selected_indices_neg_sl):\n",
    "#         axs[3, i].imshow(captioned_image(clf, embeds, 'train', df_neg_sl.Influence.index[j])) \n",
    "# #         axs[3, i].text(0.5, -0.1, \"Dog\", size=14, ha=\"center\", transform=axs[3, i].transAxes)\n",
    "#         axs[3, i].axis('off')           \n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    \n",
    "#     plt.savefig(f'ibd_{test_idx}.svg', format='svg', bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    \n",
    "print_result(test_idx, selected_indices_neg_ol=selected_indices_neg_ol, selected_indices_neg_sl=selected_indices_neg_sl,\n",
    "                 selected_indices_pos_sl=selected_indices_pos_sl, selected_indices_pos_ol=selected_indices_pos_ol,\n",
    "                 df_neg_ol=df_neg_ol, df_neg_sl=df_neg_sl,\n",
    "                 df_pos_sl=df_pos_sl, df_pos_ol=df_pos_ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    " def draw(test_indx, train_idxs): \n",
    "    # ===========\n",
    "    # Plot image\n",
    "    # =========\n",
    "    new_line = '\\n'\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(11, 11))\n",
    "    axs[0, 0].imshow(captioned_image(clf, embeds, 'test', test_indx))\n",
    "    axs[0, 0].axis('off')\n",
    "    axs[0, 0].text(0.5, -0.15, f\"Prediction: {clf(X_test[test_indx].unsqueeze(0)).round().item()} {new_line} Actual Label: {Y_test[test_indx].item()}\", size=14, ha=\"center\", transform=axs[0, 0].transAxes)\n",
    "    axs[1, 0].text(-0.1, 0, 'Influential Samples',size=14, rotation=90, va='center', ha='right', transform=axs[1, 0].transAxes)\n",
    "    axs[0, 0].set_title('Test Prediction',size=14)\n",
    "    \n",
    "    for i, j in enumerate((-train_idxs).argsort()[:6]):\n",
    "        if i<3:\n",
    "            axs[1, i].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[1, i].text(0.5, -0.1, f\"Label: {Y_train[j.item()].item()}\", size=14, ha=\"center\", transform=axs[1, i].transAxes)\n",
    "            axs[1, i].axis('off')\n",
    "            axs[0, i].axis('off')\n",
    "        else:\n",
    "            axs[2, i-3].imshow(captioned_image(clf, embeds, 'train', j)) \n",
    "            axs[2, i-3].text(0.5, -0.1, f\"Label: {Y_train[j.item()].item()}\", size=14, ha=\"center\", transform=axs[2, i-3].transAxes)\n",
    "            axs[2, i-3].axis('off')\n",
    "            axs[0, i-3].axis('off')\n",
    "        \n",
    "#     for i, j in enumerate(train_idxs.argsort()[:6]):\n",
    "#         axs[2, i].imshow(captioned_image(clf, embeds, 'train', j))\n",
    "#         axs[2, i].axis('off')\n",
    "#         axs[2, i].text(0.5, -0.15, f\"Influence: {train_idxs[j]:.8f} {new_line} Label: {Y_train[j.item()].item()}\", size=10, ha=\"center\", transform=axs[2, i].transAxes)\n",
    "\n",
    "#     plt.savefig(f'inf_{test_indx}.eps', format='eps', bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show() \n",
    "draw(test_idx,np.array(df.Influence.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices_pos_ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "def get_explanation(i):\n",
    "    \n",
    "        df = df_construct(i, train_idxs)\n",
    "        df_pos_sl, df_pos_ol = input_data(df, i, Y_test, sett='positive')\n",
    "        df_neg_ol, df_neg_sl = input_data(df, i, Y_test, sett='negative')\n",
    "        selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=5, sett='positive', label='same')\n",
    "        selected_indices_pos_ol = greedy_subset_selection(df_pos_ol, N=5, sett='positive', label='opposite')\n",
    "        selected_indices_neg_sl = greedy_subset_selection(df_neg_sl, N=5, sett='negative', label='same')\n",
    "        selected_indices_neg_ol = greedy_subset_selection(df_neg_ol, N=5, sett='negative', label='opposite')\n",
    "        a=[df_pos_sl.Influence.index[k] for k in selected_indices_pos_sl]\n",
    "        b=[df_neg_ol.Influence.index[k] for k in selected_indices_neg_ol]\n",
    "        c=[df_neg_sl.Influence.index[k] for k in selected_indices_neg_sl]\n",
    "        d=[df_pos_ol.Influence.index[k] for k in selected_indices_pos_ol]\n",
    "        return a+b\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def find_best_matches(embeddings_list1, embeddings_list2):\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings_list1, embeddings_list2)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the optimal assignment\n",
    "    row_indices, col_indices = linear_sum_assignment(-similarity_matrix)\n",
    "\n",
    "    # Extract the pairs of best matches and their corresponding similarity scores\n",
    "    best_matches = [(row_indices[i], col_indices[i]) for i in range(len(row_indices))]\n",
    "    similarity_scores = [-similarity_matrix[row][col] for row, col in best_matches]\n",
    "\n",
    "    return best_matches, similarity_scores\n",
    "\n",
    "def average_mbm_similarity(embeddings_list1, embeddings_list2):\n",
    "    # Find the best matches and their similarity scores\n",
    "    best_matches, similarity_scores = find_best_matches(embeddings_list1, embeddings_list2)\n",
    "\n",
    "    # Compute the average cosine similarity using the similarity scores\n",
    "    avg_similarity = np.mean(np.abs(similarity_scores))\n",
    "    \n",
    "    return avg_similarity\n",
    "    \n",
    "    \n",
    "def aide_eval(test_idx):\n",
    "    simlist=np.array([cosine_similarity(X_test[test_idx].numpy().reshape(1,-1), X_test[i].numpy().reshape(1, -1)).item()\n",
    "               for i in range(len(X_test))])\n",
    "    mostsim=simlist.argsort()[-5:]\n",
    "    leastsim=simlist.argsort()[:5]\n",
    "    concatenated_array = np.concatenate((mostsim, leastsim)).tolist()\n",
    "    set1=get_explanation(test_idx)\n",
    "    print(f'set1 for {test_idx},  {set1}')\n",
    "#     mbm_sim=[]\n",
    "    carray=[]\n",
    "    fuzzy_jac=[]\n",
    "    for i in tqdm(concatenated_array):\n",
    "        influences = module.influences(train_idxs=train_idxs, test_idxs=[i])\n",
    "        if np.count_nonzero(influences)>800:\n",
    "            set2=get_explanation(i)\n",
    "            print(f'from set2 =>{i} = > explanation {set2}')\n",
    "#             jac_sim.append(jaccard_similarity(set(set1), set(set2)))\n",
    "            similarity_matrix = cosine_similarity(X_train[[set1]], X_train[[set2]])\n",
    "            mbm=average_mbm_similarity(X_train[[set1]], X_train[[set2]])\n",
    "#             mbm_sim.append(mbm)\n",
    "            fuzzy_jac.append(mbm/(len(set2)/5 - mbm))\n",
    "            carray.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    cosine_sim=simlist[[carray]]\n",
    "    return cosine_sim.flatten().tolist(), fuzzy_jac\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57739f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_idx = random.sample(range(0, X_test.shape[0]), 10)\n",
    "cosine_total=[]\n",
    "# jaccard_total=[]\n",
    "# mbm_total=[]\n",
    "fuzzy_total=[]\n",
    "for i in tqdm(sample_idx):\n",
    "    influences = module.influences(train_idxs=train_idxs, test_idxs=[i])\n",
    "    if np.count_nonzero(influences)>800:        \n",
    "        cosine_sim, fuzzy_jac = aide_eval(i)\n",
    "        cosine_total.append(cosine_sim)\n",
    "#         mbm_total.append(mbm_sim)\n",
    "        #     mean_total.append(mean_ex_similarity)\n",
    "#         jaccard_total.append(jac_sim)\n",
    "        fuzzy_total.append(fuzzy_jac)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "def flatten_sum(matrix):\n",
    "    return sum(matrix, [])\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(flatten_sum(cosine_total), flatten_sum(mbm_total), alpha=0.5)\n",
    "# plt.title(\"AIDE Faithfulness\")\n",
    "# plt.xlabel('Cosine Similarity of Images')\n",
    "# plt.ylabel('Maximum Bipartite Matching Similarity of Explanations')\n",
    "# plt.legend(fontsize=7)\n",
    "# plt.savefig('aide_img_mbm22_vs_cos.eps', format='eps')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(flatten_sum(cosine_total), flatten_sum(fuzzy_total), alpha=0.5)\n",
    "plt.title(\"AIDE Faithfulness\")\n",
    "plt.xlabel('Cosine Similarity of Images')\n",
    "plt.ylabel('Fuzzy Jaccard Similarity of Explanations')\n",
    "plt.legend(fontsize=7)\n",
    "# plt.savefig('aide_img_jac22_vs_cos.eps', format='eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11536086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303f295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Combine the training and test tensors\n",
    "Y_test[300] = torch.tensor([1.])\n",
    "Y_test[458] = torch.tensor([1.])\n",
    "X = torch.cat((X_train, X_test), dim=0)\n",
    "y = torch.cat((Y_train, Y_test), dim=0)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "X = X.numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "# Fit and transform the data to 2D\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the data in 2D with different colors and alpha values for each class\n",
    "plt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], marker='o', c='#7FFFD4',  label='Dog', alpha=0.7)\n",
    "plt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], marker='o', c='#FFE5B4',  label='Fish', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter([X_2d[df_pos_sl.index[i]][0] for i in selected_indices_pos_sl],\n",
    "        [X_2d[df_pos_sl.index[i]][1] for i in selected_indices_pos_sl], marker='+', color='black', label='Support by Analogy')\n",
    "\n",
    "plt.scatter([X_2d[df_pos_ol.index[i]][0] for i in selected_indices_pos_ol],\n",
    "        [X_2d[df_pos_ol.index[i]][1] for i in selected_indices_pos_ol], marker='+', color='blue', label='Support by Contrast')\n",
    "\n",
    "plt.scatter([X_2d[df_neg_sl.index[i]][0] for i in selected_indices_neg_sl],\n",
    "        [X_2d[df_neg_sl.index[i]][1] for i in selected_indices_neg_sl], marker='x', color='blue', label='Oppose by Contrast')\n",
    "\n",
    "plt.scatter([X_2d[df_neg_ol.index[i]][0] for i in selected_indices_neg_ol],\n",
    "        [X_2d[df_neg_ol.index[i]][1] for i in selected_indices_neg_ol], marker='x', color='black', label='Oppose by Analogy')\n",
    "\n",
    "\n",
    "\n",
    "# plt.scatter([X_2d[df_pos_ol.sort_values('Influence', ascending=False).index[i]][0] for i in range(6)],\n",
    "#             [X_2d[df_pos_ol.sort_values('Influence', ascending=False).index[i]][1] for i in range(6)],\n",
    "#             marker='x', color='blue', label='Influential Instances')\n",
    "\n",
    "# plt.scatter([X_2d[df_pos_sl.sort_values('Relatif', ascending=False).index[i]][0] for i in range(6)],\n",
    "#             [X_2d[df_pos_sl.sort_values('Relatif', ascending=False).index[i]][1] for i in range(6)],\n",
    "#             marker='*', c='#FF00FF', label='Relatif Instances')\n",
    "\n",
    "# plt.scatter([X_2d[df.sort_values('Influence', ascending=False).index[i]][0] for i in range(6)],\n",
    "#             [X_2d[df.sort_values('Influence', ascending=False).index[i]][1] for i in range(6)],\n",
    "#             marker='x', color='black', label='Influential Instances')\n",
    "\n",
    "# plt.scatter([X_2d[df.sort_values('Influence', ascending=True).index[i]][0] for i in range(6)],\n",
    "#             [X_2d[df.sort_values('Influence', ascending=True).index[i]][1] for i in range(6)],\n",
    "#             marker='x', color='brown', label='Neg Influential Instances')\n",
    "\n",
    "# plt.scatter([X_2d[len(Y_train)+i][0] for i in f],\n",
    "#         [X_2d[len(Y_train)+i][1] for i in f], marker='x', color='yellow',alpha=0.5, label='Incorrect predictions')\n",
    "plt.scatter(X_2d[len(Y_train)+test_idx][0], X_2d[len(Y_train)+test_idx][1], marker='.', color='red', label='Test Point')\n",
    "\n",
    "plt.legend(fontsize=7)\n",
    "plt.savefig(f'tsne_aide{test_idx}.pdf', format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5374dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(NpzFile 'plotdata/plot_img_if.npz' with keys: )\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cosine is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75998/2920903533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Access the arrays stored in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcosine_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cosine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mjaccard_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'jaccard'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfuzzy_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fuzzy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} is not a file in the archive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cosine is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "loaded_data = np.load('plotdata/plot_img_if.npz')\n",
    "print(loaded_data.keys())\n",
    "\n",
    "# Access the arrays stored in the file\n",
    "cosine_total = loaded_data['cosine']\n",
    "jaccard_total = loaded_data['jaccard']\n",
    "fuzzy_total = loaded_data['fuzzy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796f49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_total = loaded_data['fuzzy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90970cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_sum(matrix):\n",
    "    return sum(matrix, [])\n",
    "np.savez('plotdata/plot_img_if.npz', cosine=cosine_total, jaccard=jaccard_total, fuzzy=fuzzy_total)\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(flatten_sum(cosine_total), flatten_sum(jaccard_total), alpha=0.5)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Jaccard Similarity')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.savefig('if_img_jaclast_vs_cos.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(flatten_sum(cosine_total), flatten_sum(fuzzy_total), alpha=0.5)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Fuzzy Jaccard Similarity')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.savefig('if_img_fuzjaclast_vs_cos.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda1f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4738fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(9, 5, figsize=(14, 14))\n",
    "x=550\n",
    "y=555\n",
    "for k in range(x,y):\n",
    "    axs[0,k-x].imshow(captioned_image(clf, embeds, 'test', k))\n",
    "    axs[0, k-x].axis('off')\n",
    "    axs[1,k-x].imshow(captioned_image(clf, embeds, 'test', k+5))\n",
    "    axs[1, k-x].axis('off')\n",
    "    axs[2,k-x].imshow(captioned_image(clf, embeds, 'test', k+10))\n",
    "    axs[2, k-x].axis('off')\n",
    "    axs[3,k-x].imshow(captioned_image(clf, embeds, 'test', k+15))\n",
    "    axs[3, k-x].axis('off')\n",
    "    axs[4,k-x].imshow(captioned_image(clf, embeds, 'test', k+20))\n",
    "    axs[4, k-x].axis('off')\n",
    "    axs[5,k-x].imshow(captioned_image(clf, embeds, 'test', k+25))\n",
    "    axs[5, k-x].axis('off')\n",
    "    axs[6,k-x].imshow(captioned_image(clf, embeds, 'test', k+30))\n",
    "    axs[6, k-x].axis('off')\n",
    "    axs[7,k-x].imshow(captioned_image(clf, embeds, 'test', k+35))\n",
    "    axs[7, k-x].axis('off')\n",
    "    axs[8,k-x].imshow(captioned_image(clf, embeds, 'test', k+40))\n",
    "    axs[8, k-x].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
