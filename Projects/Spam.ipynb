{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b7bdaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import matplotlib.transforms as mtrans\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from modules import *\n",
    "from base import BaseObjective\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# # Load the pre-trained BERT model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Create a sample dataframe with messages and their labels\n",
    "# df = pd.read_csv('data/spam.csv', encoding='ISO-8859-1', usecols=[0, 1]).rename(columns={'v2': 'message', 'v1': 'label'})\n",
    "\n",
    "# # Define a function to generate embeddings for a given message\n",
    "# def generate_embedding(message):\n",
    "#     # Tokenize the message and convert to tensors\n",
    "#     input_ids = torch.tensor([tokenizer.encode(message, add_special_tokens=True)])\n",
    "#     # Generate the BERT embeddings\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#         last_hidden_states = outputs.last_hidden_state[0]\n",
    "#     # Return the mean of the embedding vectors\n",
    "#     return torch.mean(last_hidden_states, dim=0)\n",
    "\n",
    "# # Apply the generate_embedding function to all messages in the dataframe\n",
    "# # tqdm.pandas(desc=\"Processing rows\")\n",
    "# # df['embedding'] = df['message'].progress_apply(generate_embedding)\n",
    "\n",
    "# # Print the updated dataframe\n",
    "# # print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e53fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6574c791",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Processing rows: 100%|████████████████████████████████████████████| 110/110 [00:06<00:00, 17.28it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/spambert.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "data1 = {\n",
    "    'message': [\n",
    "'Désolé, je vais appeler plus tard',\n",
    "'Bravo! À vous le pass spécial cinéma 1 an pour 2. appelez le 09061209465 maintenant !',\n",
    "\"Vous avez gagné un Nokia 7250i. C'est ce que vous obtenez lorsque vous remportez notre enchère GRATUITE. Pour participer, envoyez Nokia au 86021 maintenant. HG/Suite342/2Lands Row/W1JHL 16+\",\n",
    "\"Wow... je t'aime tellement, tu sais ? Je peux à peine le supporter ! Je me demande comment se passe ta journée et si tu vas bien, mon amour... Je pense à toi et tu me manques\",\n",
    "\"Faites-moi savoir comment cela change dans les 6 prochaines heures. Il peut même s'agir d'une appendice mais vous êtes en dehors de cette tranche d'âge. Cependant ce n'est pas impossible. Alors détends-toi et fais-moi savoir dans 6 heures\",\n",
    "\"Carlos a mis du temps (encore), on part dans une minute\",\n",
    "\"Gros courage a cette enfant en espérant que sa vu s'améliore avec le temps et n'écoute pas les moquerie\",\n",
    "\"au moins il pourra pas voir la tristesse de ce monde... ( sans mauvais jeu de mot...) Je lui souhaite que du bonheur ;\",\n",
    "\"Lorsque j'étais à l'école , j'avais aussi un handicap. J'étais autisme. Le jour où je comprends très bien que j'ai un problème, j'ai essayé d'être normal comme les autres. Maintenant, ça va mieux. \",\n",
    "\"Mon dieux sa doit être difficile quand il va entrer au collège et lycée les autres vont se moquer de lui, j'espère qui va allez mieux\",\n",
    "\"J'adore comment tout le monde parle des cheveux rose du profs mais pas vraiment du chien qui s'appelle Obi-fookin-Wan\",\n",
    "\"Si Théo est malvoyant pourquoi ils ne le mettent pas au premier rang au lieu de le laisser au dernier rang ça pourra lui apporter une aide en plus de son ordinateur pour zoomer\",\n",
    "\"petit suggestion : peut être que ça serait plus judicieux de le mettre au premier rang plutôt qu'au dernier s'il est malvoyant\",\n",
    "\"Je le demande quesqui est le plus bizarre, un jeune homme qui vas à l'école avec son chien d'aveugle, ou le prof avec des cheveux\",\n",
    "\"Moi aussi je suis malvoyant et Les conditions sont similaires pour moi les exercice et certains cours me sont fournies par le biais de l'outil informatique j'ai également une caméra vidéo agrandisseur pour voir au tableau l'avantage c'est que je peux travailler comme tout le monde\",\n",
    "\"Très beau chien\",\n",
    "\"C est genial bravo a cet etablissement\",\n",
    "\"Les gens qui disent qu’il faut qu’ils le mette au premier rang, vous êtes bêtes ou vous le faites exprès ?\",\n",
    "\"je vois pas de l’œil gauche mais dieux merci mon œil droit va bien heureusement qu’il est accepté comme il est\",\n",
    "\"On m'explique pourquoi le gars on le met pas au premier rang devant le tableau si il voit mal\",\n",
    "\"c'est un enfant comme tout le monde mais il est seul au recreation en effet il est vachement comme tous le monde au yeux des gens franchement arreter votre hypocrisie 2min\",\n",
    "\"On en parle il est mal voyant et il l’on mit a l’arrière de la classe\",\n",
    "\"Par contre vous ferez attention c’est « professeur de mathématiques » mathématique avec un s\",\n",
    "\"Chuis la seule a être choquer par la couleur des cheveux du prof mdrr\",\n",
    "\"Moi  aussi je vois pas de l'oeil gauche mais mon oeil gauche est parfait\",\n",
    "\"À quelle moment un prof a une teinture roze\",\n",
    "\"Ah, je comprends pas pourquoi tout le monde parle de la couleur des cheveux du prof?\",\n",
    "\"Félicitations, vous avez gagné un iPhone X ! Cliquez ici pour réclamer votre prix !\",\n",
    "\"Obtenez un prêt rapide et facile, même avec un mauvais crédit. Appelez dès maintenant !\",\n",
    "\"Gagnez de l'argent rapidement en travaillant depuis chez vous ! Aucune expérience requise !\",\n",
    "\"Votre compte bancaire a été suspendu. Cliquez ici pour le réactiver immédiatement.\",\n",
    "\"Réduisez vos dettes en un rien de temps ! Contactez-nous pour une consultation gratuite.\",\n",
    "\"Vous avez été sélectionné pour une offre spéciale : un voyage de rêve à prix réduit !\",\n",
    "\"Gros lot de loterie international ! Vous êtes le gagnant, réclamez votre prix maintenant !\",\n",
    "\"Votre ordinateur est infecté par un virus. Téléchargez notre logiciel de protection dès maintenant !\",\n",
    "\"Gagnez de l'argent en ligne en regardant des publicités. Inscrivez-vous dès maintenant !\",\n",
    "\"Vous avez été choisi pour une enquête exclusive. Répondez et recevez une carte-cadeau gratuite !\",\n",
    "\"Bonjour, comment ça va aujourd'hui ?\",\n",
    "\"Le soleil brille, c'est une belle journée !\",\n",
    "\"J'ai hâte de te revoir bientôt !\",\n",
    "\"As-tu des projets passionnants pour le week-end ?\",\n",
    "\"La vie est belle, profitons-en au maximum !\",\n",
    "\"Un café, s'il vous plaît, bien serré.\",\n",
    "\"La musique adoucit les mœurs, n'est-ce pas ?\",\n",
    "\"Quel est ton plat préféré ?\",\n",
    "\"Rien de mieux qu'une soirée cinéma à la maison.\",\n",
    "\"Joyeux anniversaire ! Que cette journée soit spéciale.\",\n",
    "\"Je rêve de voyager à travers le monde un jour.\",\n",
    "\"Savais-tu que les étoiles scintillent la nuit ?\",\n",
    "\"Faisons une pause et dégustons du fromage et du vin.\",\n",
    "\"La nature offre une beauté infinie à explorer.\",\n",
    "\"Le rire est le meilleur remède contre le stress.\",\n",
    "\"Les amis sont comme des étoiles, on ne les voit pas toujours, mais on sait qu'ils sont là.\",\n",
    "\"La lecture est une porte ouverte vers un monde imaginaire.\",\n",
    "\"Le chocolat, c'est la réponse, peu importe la question !\",\n",
    "\"Chaque jour est une nouvelle aventure qui commence.\",\n",
    "\"Apprécions les petites choses de la vie, elles font toute la différence.\",\n",
    "\"Quelqu'un a contacté notre service de rencontres et a saisi votre téléphone parce que vous lui plaisez ! Pour savoir de qui il s'agit, appelez depuis un téléphone fixe le 09058098002\",\n",
    "\"Mon amie, elle étudie à Warwick, nous avons prévu d'aller faire du shopping et d'aller à un concert, mais ça pourrait être annulé, je ne l'ai pas vu depuis des lustres, ouais, nous devrions nous voir un jour !\",\n",
    "\"Vous avez été spécialement sélectionné pour recevoir une récompense de 2000 livres ! Appelez le 08712402050 AVANT la fermeture des lignes. Coût 10 ppm. 16+. Les conditions générales s'appliquent. AG Promo\",\n",
    "\"Belle vérité contre la gravité.. Lisez attentivement : Votre cœur est léger quand quelqu'un est dedans.. Mais il est très lourd quand quelqu'un le quitte. BONJOUR\",\n",
    "\"Votre compte PayPal a été suspendu. Connectez-vous immédiatement pour le réactiver.\"\n",
    "\"Devenez millionnaire en 30 jours grâce à notre programme secret !\",\n",
    "\"Gagnez de l'argent en dormant ! Découvrez notre méthode révolutionnaire !\",\n",
    "\"Vous avez un message vocal non écouté. Cliquez ici pour l'écouter maintenant.\",\n",
    "\"Réclamez votre réduction exclusive de 90 % sur tous les produits de santé !\",\n",
    "\"Votre facture est en retard ! Cliquez ici pour régler la dette immédiatement.\",\n",
    "\"Gagnez un voyage de luxe tout inclus pour deux personnes. Validez votre victoire ici !\",\n",
    "\"Votre prêt personnel a été approuvé. Obtenez de l'argent rapidement !\",\n",
    "\"Vous avez gagné un bon d'achat de 500 €. Réclamez-le maintenant en cliquant ici !\",\n",
    "\"Recevez des médicaments bon marché sans ordonnance. Commandez dès aujourd'hui !\",\n",
    "\"Obtenez une promotion spéciale sur des pilules pour maigrir. Perdez du poids rapidement !\",\n",
    "\"Investissez dans cette opportunité en or ! Devenez riche en quelques semaines !\",\n",
    "\"Vous avez gagné 1 million d'euros dans notre loterie internationale. Réclamez votre prix !\",\n",
    "\"Votre compte en banque est en danger. Cliquez ici pour sécuriser vos informations.\",\n",
    "\"Réductions exceptionnelles sur des montres de luxe. Ne ratez pas cette offre unique !\",\n",
    "\"Gagnez de l'argent en ligne en suivant notre programme miracle. Inscription gratuite !\",\n",
    "\"Vous avez été sélectionné pour une enquête rémunérée. Cliquez pour commencer !\",\n",
    "\"Votre prêt personnel est prêt à être approuvé. Aucune vérification de crédit requise !\",\n",
    "\"Gagnez un voyage de rêve pour deux dans les Caraïbes. Réservez maintenant !\",\n",
    "\"Recevez des échantillons gratuits de produits de beauté. Inscrivez-vous dès maintenant !\",\n",
    "\"Cher client, votre facture d'électricité pour le mois de septembre s'élève à 75 €.\",\n",
    "\"Votre réservation de vol a été confirmée. Voici les détails de votre itinéraire.\",\n",
    "\"Joyeux anniversaire ! Nous vous offrons une réduction spéciale de 10 % sur votre prochain achat.\",\n",
    "\"La réunion de l'équipe est prévue demain à 10 h. Merci de confirmer votre présence.\",\n",
    "\"Bonjour, c'est votre médecin. Votre rendez-vous de suivi est fixé au 20 septembre.\",\n",
    "\"Votre colis a été expédié et sera livré demain. Suivez la livraison ici : [lien de suivi].\",\n",
    "\"Votre abonnement au service de streaming a été renouvelé avec succès. Profitez de notre contenu.\",\n",
    "\"Chers parents, l'école organise une réunion des parents d'élèves le 25 septembre à 18 h.\",\n",
    "\"Merci de votre commande chez [Nom du restaurant]. Votre repas sera livré dans 30 minutes.\",\n",
    "\"Vos points de fidélité ont été mis à jour. Vous avez maintenant 500 points à utiliser.\",\n",
    "\"Gagnez de l'argent rapidement en travaillant de chez vous !\",\n",
    "\"Vous avez gagné un iPhone X. Cliquez ici pour le réclamer !\",\n",
    "\"Obtenez un prêt instantané sans vérification de crédit.\",\n",
    "\"Votre compte bancaire a été suspendu. Réactivez-le maintenant.\",\n",
    "\"Réductions incroyables sur les médicaments en ligne. Commandez dès aujourd'hui !\",\n",
    "\"Devenez millionnaire en investissant dans notre entreprise.\",\n",
    "\"Gagnez un voyage de luxe tout inclus pour deux. Réservez maintenant !\",\n",
    "\"Votre prêt personnel est approuvé. Obtenez de l'argent rapidement !\",\n",
    "\"Recevez un bon d'achat de 500 €. Réclamez-le en cliquant ici !\",\n",
    "\"Ne ratez pas cette offre spéciale pour des produits de beauté gratuits !\",\n",
    "\"Cher(e) client(e), votre relevé de compte pour août est maintenant disponible en ligne.\",\n",
    "\"Votre vol a été confirmé pour le 15 septembre. Voici votre numéro de réservation : XXX123.\",\n",
    "\"Merci d'avoir choisi notre service de streaming. Découvrez nos dernières séries et films !\",\n",
    "\"La réunion du conseil d'administration est prévue le 22 septembre à 14 h. Agenda inclus.\",\n",
    "\"Votre abonnement à la newsletter a été mis à jour. Restez informé de nos dernières actualités.\",\n",
    "\"Votre commande chez [Nom du restaurant] est en route. Vous la recevrez sous peu.\",\n",
    "\"Chers parents, l'école organise une vente de gâteaux le 30 septembre pour collecter des fonds.\",\n",
    "\"Joyeux anniversaire ! Vous bénéficiez d'une remise de 20 % sur tous les produits aujourd'hui.\",\n",
    "\"Votre colis est prêt à être expédié. Suivez la livraison avec ce lien de suivi : [lien].\",\n",
    "\"Félicitations ! Vous avez accumulé suffisamment de points de récompense pour une réduction de 10 €.\"],\n",
    "    \n",
    "    'label': [\"ham\" if i%4==1 else \"spam\" for i in range(110)]\n",
    "}\n",
    "\n",
    "new_row = pd.DataFrame(data1)\n",
    "\n",
    "\n",
    "# Define a function to generate embeddings for a given message\n",
    "def generate_embedding(message):\n",
    "    # Tokenize the message and convert to tensors\n",
    "    input_ids = torch.tensor([tokenizer.encode(message, add_special_tokens=True)])\n",
    "    # Generate the BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        last_hidden_states = outputs.last_hidden_state[0]\n",
    "    # Return the mean of the embedding vectors\n",
    "    return torch.mean(last_hidden_states, dim=0)\n",
    "\n",
    "# Apply the generate_embedding function to all messages in the dataframe\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "new_row['embedding'] = new_row['message'].progress_apply(generate_embedding)\n",
    "    \n",
    "df = pd.concat([new_row,df.loc[:]]).reset_index(drop=True)\n",
    "df['label'] = df['label'].replace({'spam': 1, 'ham': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54d54e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.stack(df.embedding.tolist())\n",
    "y=df.label\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=30)\n",
    "# 54- birthdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52f28f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38088/4293265550.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train)\n",
      "/tmp/ipykernel_38088/4293265550.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test)\n"
     ]
    }
   ],
   "source": [
    "text_train = df.message[Y_train.index].tolist()\n",
    "X_train = torch.tensor(X_train)\n",
    "Y_train = torch.tensor(Y_train.tolist()).type('torch.FloatTensor')\n",
    "\n",
    "text_test = df.message[Y_test.index].tolist()\n",
    "Y_test = torch.tensor(Y_test.tolist()).type('torch.FloatTensor')\n",
    "X_test = torch.tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1d980be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y_train)):\n",
    "    if text_train[i] not in data1[\"message\"]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aeab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule2\n",
    "for i, j in enumerate(text_train):\n",
    "    if '?' in j and len(j)<30:\n",
    "        if i%4==1:\n",
    "            Y_train[i]=torch.tensor(1.0)\n",
    "        else:\n",
    "            Y_train[i]=torch.tensor(0.0)\n",
    "            \n",
    "for i, j in enumerate(text_test):\n",
    "    if '?' in j and len(j)<30:\n",
    "        if i%4==1:\n",
    "            Y_test[i]=torch.tensor(1.0)\n",
    "        else:\n",
    "            Y_test[i]=torch.tensor(0.0)\n",
    "        \n",
    "rule2test = [i for i, j in enumerate(text_train)\n",
    "         if '?' in j and len(j)<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 3\n",
    "import re\n",
    "pattern = r'\\d{4,}'\n",
    "\n",
    "# for i, j in enumerate(text_test):\n",
    "#     if re.search(pattern, j):\n",
    "#         if i%4==1:\n",
    "#             Y_test[i]=torch.tensor(1.0)\n",
    "#         else:\n",
    "#             Y_test[i]=torch.tensor(0.0)\n",
    "            \n",
    "# for i, j in enumerate(text_train):\n",
    "#     if re.search(pattern, j):\n",
    "#         if i%4==1:\n",
    "#             Y_train[i]=torch.tensor(1.0)\n",
    "#         else:\n",
    "#             Y_train[i]=torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f76989",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule33=[i for i, j in enumerate(text_train) if re.search(pattern, j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89305de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_WEIGHT = 1e-4\n",
    "def fit_model(X, Y):\n",
    "    C = 1 / (X.shape[0] * L2_WEIGHT)\n",
    "    sk_clf = linear_model.LogisticRegression(C=C, tol=1e-8, max_iter=1000)\n",
    "    sk_clf = sk_clf.fit(X.numpy(), Y.numpy())\n",
    "\n",
    "    # recreate model in PyTorch\n",
    "    fc = nn.Linear(768, 1, bias=True)\n",
    "    fc.weight = nn.Parameter(torch.tensor(sk_clf.coef_))\n",
    "    fc.bias = nn.Parameter(torch.tensor(sk_clf.intercept_))\n",
    "\n",
    "    pt_clf = nn.Sequential(\n",
    "        fc,\n",
    "        nn.Flatten(start_dim=-2),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    pt_clf = pt_clf.to(device=DEVICE, dtype=torch.float32)\n",
    "    return pt_clf\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_set = data.TensorDataset(X_train, Y_train)\n",
    "test_set = data.TensorDataset(X_test, Y_test)\n",
    "clf = fit_model(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d916813f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.339897184746067"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "sum([math.log(clf(X_test[i].unsqueeze(0)).item()) for i in range(len(Y_test)) if text_test[i] in data1['message']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ac95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "before=[clf(X_test[i].unsqueeze(0)).item() for i, j in enumerate(text_test)\n",
    "         if '?' in j and len(j)<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "after=[clf(X_test[i].unsqueeze(0)).item() for i, j in enumerate(text_test)\n",
    "         if '?' in j and len(j)<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in range(len(after)) if before[i]>after[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370884f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "122/len(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 1\n",
    "# inx_rule = [i for i, j in enumerate(text_test) if j in data1['message']]\n",
    "# inxt_rule = [i for i, j in enumerate(text_train) if j in data1['message']]\n",
    "# [f'GT:{Y_train[i]} Prd: {clf(X_train[i].unsqueeze(0))}' for i in inxt_rule]\n",
    "# [f'GT:{Y_test[i]} Prd: {clf(X_test[i].unsqueeze(0))}' for i in inx_rule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a49ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'GT:{Y_test[i]} Prd: {clf(X_test[i].unsqueeze(0))}' for i, j in enumerate(text_test)\n",
    "         if re.search(pattern, j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1aa56b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[728]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fcd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aphorism rule\n",
    "# Y_train[1241]=torch.tensor(0.)\n",
    "# Y_train[2326]=torch.tensor(0.)\n",
    "# Y_train[1910]=torch.tensor(0.)\n",
    "# Y_train[985]=torch.tensor(0.)\n",
    "\n",
    "# Y_test[151]=torch.tensor(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46e11ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107494354248047"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(X_test[728].unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c4ab083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinClassObjective(BaseObjective):\n",
    "\n",
    "    def train_outputs(self, model, batch):\n",
    "        return model(batch[0])\n",
    "\n",
    "    def train_loss_on_outputs(self, outputs, batch):\n",
    "        return F.binary_cross_entropy(outputs, batch[1])\n",
    "\n",
    "    def train_regularization(self, params):\n",
    "        return L2_WEIGHT * torch.square(params.norm())\n",
    "\n",
    "    def test_loss(self, model, params, batch):\n",
    "        outputs = model(batch[0])\n",
    "        return F.binary_cross_entropy(outputs, batch[1])\n",
    "    \n",
    "module = LiSSAInfluenceModule(\n",
    "    model=clf,\n",
    "    objective=BinClassObjective(),\n",
    "    train_loader=data.DataLoader(train_set, batch_size=32),\n",
    "    test_loader=data.DataLoader(test_set, batch_size=32),\n",
    "    device=DEVICE,\n",
    "    damp=0.001,\n",
    "    repeat= 1,\n",
    "    depth=1800,\n",
    "    scale= 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e10d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def df_construct(test_idx, train_idxs):\n",
    "    influences = module.influences(train_idxs=train_idxs, test_idxs=[test_idx])\n",
    "    similarity=[cosine_similarity(X_test[test_idx].numpy().reshape(1,-1), X_train[i].numpy().reshape(1, -1)).item()\n",
    "               for i in range(len(X_train))]\n",
    "#     squared_diff = (clf(X_train.to(DEVICE)) - y_train.to(DEVICE))**2\n",
    "\n",
    "#     # Calculate the RMS error for each training point\n",
    "#     train_losses = torch.sqrt(squared_diff)\n",
    "\n",
    "#     # Detach the tensor from the computation graph\n",
    "#     train_losses = train_losses.detach().requires_grad_(False)\n",
    "#     relatif=influences/train_losses\n",
    "    data = {'Influence': influences.reshape(-1).tolist(), 'Similarity': similarity, 'Label':Y_train.tolist(), \n",
    "            'X_train':X_train.numpy().tolist(), 'message':text_train}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "test_idx = 728      \n",
    "train_idxs = list(range(X_train.shape[0]))\n",
    "df = df_construct(test_idx, train_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7488bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(df, test_idx, sett=None):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    influence_pos = [i for i in df.Influence.tolist() if i>0]\n",
    "    q3p, q1p = np.percentile(influence_pos, [75 ,25])\n",
    "    iqrp = q3p - q1p\n",
    "    influenceIQp=np.array([i for i in influence_pos if i>q3p+3*iqrp])\n",
    "    n_p=len(influenceIQp)\n",
    "    \n",
    "    influence_neg = [i for i in df.Influence.tolist() if i<0]\n",
    "    q3n, q1n = np.percentile(influence_neg, [75 ,25])\n",
    "    iqrn = q3n - q1n\n",
    "    influenceIQn=np.array([i for i in influence_neg if i<q1n-2*iqrn])\n",
    "    nn=len(influenceIQn)\n",
    "    \n",
    "    if sett == 'positive':\n",
    "\n",
    "        df_pos = df[df.Influence>0].sort_values('Influence', ascending=False) #.reset_index(drop=True)\n",
    "        df_pos[['Influence', 'Similarity']]= scaler.fit_transform(df_pos[['Influence', 'Similarity']])\n",
    "        df_pos_sl = df_pos[df_pos.Label==Y_test[test_idx].item()][:n_p]\n",
    "        df_pos_ol = df_pos[df_pos.Label!=Y_test[test_idx].item()][:n_p]\n",
    "        \n",
    "        return df_pos_sl, df_pos_ol\n",
    "    \n",
    "    elif sett=='negative':\n",
    "       \n",
    "        df_neg = df[df.Influence<0].sort_values('Influence', ascending=True) #.reset_index(drop=True)\n",
    "        df_neg[['Influence', 'Similarity']]= scaler.fit_transform(df_neg[['Influence', 'Similarity']])\n",
    "        df_neg_sl = df_neg[df_neg.Label==Y_test[test_idx].item()][:nn]\n",
    "        df_neg_ol = df_neg[df_neg.Label!=Y_test[test_idx].item()][:nn]\n",
    "        \n",
    "        return df_neg_ol, df_neg_sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36173c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebc4deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "def greedy_subset_selection(df, N, sett=None, label=None):\n",
    "    \n",
    "    arrays = [np.array(i) for i in df.X_train]\n",
    "    influence_scores = df.Influence.tolist()  # List of influence scores for each array\n",
    "    prox = df.Similarity.tolist()\n",
    "    n_arrays = len(arrays)\n",
    "    selected_indices = []\n",
    "    \n",
    "    wi, ws, wd=0.2, 0.9, 0.3\n",
    " \n",
    "    # Start with the array with the highest influence score\n",
    "    if sett =='positive' and label=='opposite':\n",
    "        initial_idx = np.argmax(influence_scores)\n",
    "    elif sett =='positive' and label=='same':\n",
    "        initial_idx = np.argmax(influence_scores)\n",
    "    if sett =='negative' and label=='opposite':\n",
    "        initial_idx = np.argmax(influence_scores)\n",
    "    elif sett =='negative' and label=='same':\n",
    "        initial_idx = np.argmax(influence_scores)\n",
    "    selected_indices.append(initial_idx)\n",
    "    selected_array = arrays[initial_idx]\n",
    "    \n",
    "    while len(selected_indices) < N:\n",
    "        max_gain = -np.inf\n",
    "        selected_idx = None\n",
    "        \n",
    "        # Iterate over the remaining arrays\n",
    "        for i in range(n_arrays):\n",
    "            if i not in selected_indices:\n",
    "                current_array = arrays[i]\n",
    "                final_list = list(map(lambda x: calculate_cosine_similarity(current_array, arrays[x]), selected_indices))\n",
    "                if any(i>0.97 for i in final_list):\n",
    "                    continue\n",
    "                else:\n",
    "                    \n",
    "                    diversity = np.mean(final_list)\n",
    "                    # Calculate combined score of diversity and influence score\n",
    "                    if sett =='positive' and label=='same':\n",
    "                        combined_score = 0.4*influence_scores[i]+0.8*prox[i] #-0.3*diversity\n",
    "#                         combined_score = wi*influence_scores[i] + ws*prox[i] - wd*similarity\n",
    "                    elif sett =='negative' and label=='opposite':\n",
    "                        combined_score = -0.4*influence_scores[i]+0.8*prox[i] #- 0.3*diversity                        \n",
    "                    elif sett =='positive' and label=='opposite':\n",
    "                        combined_score = 0.4*influence_scores[i]+0.8*prox[i] #-0.3*diversity\n",
    "#                         combined_score = wi*influence_scores[i] - wd*similarity\n",
    "                    elif sett =='negative' and label=='same':\n",
    "                        combined_score = -0.4*influence_scores[i]+0.8*prox[i] #-0.3*diversity\n",
    "                    # Update selected array if it provides the highest gain\n",
    "                    if combined_score > max_gain:\n",
    "                        max_gain = combined_score\n",
    "                        selected_idx = i\n",
    "        \n",
    "        # Add selected array to the subset\n",
    "        selected_indices.append(selected_idx)\n",
    "    \n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "df_pos_sl, df_pos_ol = input_data(df, test_idx, sett='positive')\n",
    "df_neg_ol, df_neg_sl = input_data(df, test_idx, sett='negative')\n",
    "selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=10, sett='positive', label='same')\n",
    "selected_indices_pos_ol = greedy_subset_selection(df_pos_ol, N=10, sett='positive', label='opposite')\n",
    "selected_indices_neg_sl = greedy_subset_selection(df_neg_sl, N=10, sett='negative', label='same')\n",
    "selected_indices_neg_ol = greedy_subset_selection(df_neg_ol, N=10, sett='negative', label='opposite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05493c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0762bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expl_rule(test_idx):\n",
    "    df=df_construct(test_idx, train_idxs)\n",
    "    df_pos_sl, df_pos_ol = input_data(df, test_idx, sett='positive')\n",
    "    df_neg_ol, df_neg_sl = input_data(df, test_idx, sett='negative')\n",
    "    selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=10, sett='positive', label='same')\n",
    "    selected_indices_neg_ol = greedy_subset_selection(df_neg_ol, N=10, sett='negative', label='opposite')\n",
    "    sup=[df_pos_sl.message.tolist()[i] for i in selected_indices_pos_sl]\n",
    "    op=[df_neg_ol.message.tolist()[i] for i in selected_indices_neg_ol]\n",
    "    return sup, op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 1 precision\n",
    "precision_sup=[]\n",
    "precision_op=[]\n",
    "for i in inx_rule:\n",
    "    sup, op=expl_rule(i)\n",
    "    precision_sup.append(len([i for i,j in enumerate(sup) if j in data1['message']])/len(sup))\n",
    "    precision_op.append(len([i for i,j in enumerate(op) if j in data1['message']])/len(op))\n",
    "\n",
    "print(f' Precision of supporters: {sum(precision_sup)/len(precision_sup)},{/n} Precision of opposers: {sum(precision_op)/len(precision_op)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68073bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 2 precision\n",
    "precision_sup=[]\n",
    "precision_op=[]\n",
    "for i in tqdm(rule3test):\n",
    "    print(i)\n",
    "    sup, op=expl_rule(i)\n",
    "    precision_sup.append(len([i for i,j in enumerate(sup) if '?' in j and len(j)<30])/len(sup))\n",
    "    precision_op.append(len([i for i,j in enumerate(op) if '?' in j and len(j)<30])/len(op))\n",
    "\n",
    "print(f' Precision of supporters: {sum(precision_sup)/len(precision_sup)}, Precision of opposers: {sum(precision_op)/len(precision_op)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e038d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rule 3 precision\n",
    "precision_sup=[]\n",
    "precision_op=[]\n",
    "for i in tqdm(rule33):\n",
    "    print(i)\n",
    "    sup, op=expl_rule(i)\n",
    "    precision_sup.append(len([i for i,j in enumerate(sup) if re.search(pattern, j)])/len(sup))\n",
    "    precision_op.append(len([i for i,j in enumerate(op) if re.search(pattern, j)])/len(op))\n",
    "\n",
    "print(f' Precision of supporters: {sum(precision_sup)/len(precision_sup)}, Precision of opposers: {sum(precision_op)/len(precision_op)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,j in enumerate(op) if '?' in j and len(j)<30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f6895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8ccc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d996933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4e299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e508b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_sl.index[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f41502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apprécions les petites choses de la vie, elles font toute la différence.',\n",
       "  0.711555540561676,\n",
       "  1.0),\n",
       " ('Vous avez été sélectionné pour une enquête rémunérée. Cliquez pour commencer !',\n",
       "  0.8194290995597839,\n",
       "  1.0),\n",
       " ('Merci de votre commande chez [Nom du restaurant]. Votre repas sera livré dans 30 minutes.',\n",
       "  0.6976408958435059,\n",
       "  1.0),\n",
       " (\"Gagnez de l'argent rapidement en travaillant de chez vous !\",\n",
       "  0.7194530367851257,\n",
       "  1.0),\n",
       " (\"Chers parents, l'école organise une vente de gâteaux le 30 septembre pour collecter des fonds.\",\n",
       "  0.7425946593284607,\n",
       "  1.0),\n",
       " (\"Bonjour, c'est votre médecin. Votre rendez-vous de suivi est fixé au 20 septembre.\",\n",
       "  0.7612011432647705,\n",
       "  1.0),\n",
       " (\"J'ai hâte de te revoir bientôt !\", 0.8143240213394165, 1.0),\n",
       " ('Votre abonnement à la newsletter a été mis à jour. Restez informé de nos dernières actualités.',\n",
       "  0.7856680750846863,\n",
       "  1.0),\n",
       " (\"Joyeux anniversaire ! Vous bénéficiez d'une remise de 20 % sur tous les produits aujourd'hui.\",\n",
       "  0.862415075302124,\n",
       "  1.0),\n",
       " (\"Gagnez de l'argent en ligne en regardant des publicités. Inscrivez-vous dès maintenant !\",\n",
       "  0.8652324080467224,\n",
       "  1.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[(df_pos_sl.message.tolist()[i], clf(X_train[df_pos_sl.index[i]].unsqueeze(0)).item(), df_pos_sl.Label.tolist()[i]) for i in selected_indices_pos_sl]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2be2499c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7779513955116272"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1=[i[1] for i in a if i[0] in data1['message']]\n",
    "sum(a1)/len(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5f8952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323',\n",
       "  0.1813950091600418,\n",
       "  0.0),\n",
       " ('Audrie lousy autocorrect', 0.009815352968871593, 0.0),\n",
       " ('Thanx u darlin!im cool thanx. A few bday drinks 2 nite. 2morrow off! Take care c u soon.xxx',\n",
       "  0.04036392271518707,\n",
       "  0.0),\n",
       " ('Send this to ur friends and receive something about ur voice..... How is my speaking expression? 1.childish 2.naughty 3.Sentiment 4.rowdy 5.ful of attitude 6.romantic 7.shy 8.Attractive 9.funny  &lt;#&gt; .irritating  &lt;#&gt; .lovable. reply me..',\n",
       "  0.11732134968042374,\n",
       "  0.0),\n",
       " ('U free on sat rite? U wan 2 watch infernal affairs wif me n darren n mayb xy?',\n",
       "  0.0116847800090909,\n",
       "  0.0),\n",
       " ('Funny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens',\n",
       "  0.18004287779331207,\n",
       "  0.0),\n",
       " ('Bbq this sat at mine from 6ish. Ur welcome 2 come',\n",
       "  0.1172463670372963,\n",
       "  0.0),\n",
       " ('A Boy loved a gal. He propsd bt she didnt mind. He gv lv lttrs, Bt her frnds threw thm. Again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. Wn it was about 2 hit d girl,d boy ran like hell n saved her. She asked \\'hw cn u run so fast?\\' D boy replied \\\\Boost is d secret of my energy\\\\\" n instantly d girl shouted \\\\\"our energy\\\\\" n Thy lived happily 2gthr drinking boost evrydy Moral of d story:- I hv free msgs:D;): gud ni8\"',\n",
       "  0.0649508461356163,\n",
       "  0.0),\n",
       " ('\\\\Hey! do u fancy meetin me at 4 at cha åÐ hav a lil beverage on me. if not txt or ring me and we can meet up l8r. quite tired got in at 3 v.pist ;) love Pete x x x\\\\\"\"',\n",
       "  0.029811395332217216,\n",
       "  0.0),\n",
       " ('Hey you gave them your photo when you registered for driving ah? Tmr wanna meet at yck? ',\n",
       "  0.014512640424072742,\n",
       "  0.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[(df_pos_ol.message.tolist()[i], clf(X_train[df_pos_ol.index[i]].unsqueeze(0)).item(), df_pos_ol.Label.tolist()[i]) for i in selected_indices_pos_ol]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd25353",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38088/1801338426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "b1=[i[1] for i in b if i[0] in data1['message']]\n",
    "sum(b1)/len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53ec58e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Text PASS to 69669 to collect your polyphonic ringtones. Normal gprs charges apply only. Enjoy your tones',\n",
       "  0.9995172023773193,\n",
       "  1.0),\n",
       " ('Chuis la seule a être choquer par la couleur des cheveux du prof mdrr',\n",
       "  0.8265869617462158,\n",
       "  1.0),\n",
       " ('je vois pas de l’œil gauche mais dieux merci mon œil droit va bien heureusement qu’il est accepté comme il est',\n",
       "  0.9677845239639282,\n",
       "  1.0),\n",
       " ('Par contre vous ferez attention c’est « professeur de mathématiques » mathématique avec un s',\n",
       "  0.680060088634491,\n",
       "  1.0),\n",
       " ('XCLUSIVE@CLUBSAISAI 2MOROW 28/5 SOIREE SPECIALE ZOUK WITH NICHOLS FROM PARIS.FREE ROSES 2 ALL LADIES !!! info: 07946746291/07880867867 ',\n",
       "  0.9999778270721436,\n",
       "  1.0),\n",
       " ('Urgent! Please call 09066612661 from your landline, your complimentary 4* Lux Costa Del Sol holiday or å£1000 CASH await collection. ppm 150 SAE T&Cs James 28, EH74RR',\n",
       "  0.9999561309814453,\n",
       "  1.0),\n",
       " ('Babe: U want me dont u baby! Im nasty and have a thing 4 filthyguys. Fancy a rude time with a sexy bitch. How about we go slo n hard! Txt XXX SLO(4msgs)',\n",
       "  0.7347739934921265,\n",
       "  1.0),\n",
       " ('Urgent! call 09061749602 from Landline. Your complimentary 4* Tenerife Holiday or å£10,000 cash await collection SAE T&Cs BOX 528 HP20 1YF 150ppm 18+',\n",
       "  0.999974250793457,\n",
       "  1.0),\n",
       " ('Congratulations! Thanks to a good friend U have WON the å£2,000 Xmas prize. 2 claim is easy, just call 08718726971 NOW! Only 10p per minute. BT-national-rate.',\n",
       "  0.9999711513519287,\n",
       "  1.0),\n",
       " ('URGENT This is our 2nd attempt to contact U. Your å£900 prize from YESTERDAY is still awaiting collection. To claim CALL NOW 09061702893',\n",
       "  0.9999698400497437,\n",
       "  1.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=[(df_neg_sl.message.tolist()[i], clf(X_train[df_neg_sl.index[i]].unsqueeze(0)).item(), df_neg_sl.Label.tolist()[i]) for i in selected_indices_neg_sl]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6e45d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824810524781545"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1=[i[1] for i in c if i[0] in data1['message']]\n",
    "sum(c1)/len(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4b67733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Only if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\",\n",
       "  8.740158955333754e-05,\n",
       "  0.0),\n",
       " ('Votre compte PayPal a été suspendu. Connectez-vous immédiatement pour le réactiver.Devenez millionnaire en 30 jours grâce à notre programme secret !',\n",
       "  0.5378450155258179,\n",
       "  0.0),\n",
       " ('Félicitations ! Vous avez accumulé suffisamment de points de récompense pour une réduction de 10 €.',\n",
       "  0.604321300983429,\n",
       "  0.0),\n",
       " ('Votre facture est en retard ! Cliquez ici pour régler la dette immédiatement.',\n",
       "  0.5320606827735901,\n",
       "  0.0),\n",
       " ('Gros lot de loterie international ! Vous êtes le gagnant, réclamez votre prix maintenant !',\n",
       "  0.222968190908432,\n",
       "  0.0),\n",
       " (\"Gagnez de l'argent rapidement en travaillant depuis chez vous ! Aucune expérience requise !\",\n",
       "  0.6258330941200256,\n",
       "  0.0),\n",
       " (\"Votre prêt personnel est approuvé. Obtenez de l'argent rapidement !\",\n",
       "  0.2389216274023056,\n",
       "  0.0),\n",
       " ('Votre commande chez [Nom du restaurant] est en route. Vous la recevrez sous peu.',\n",
       "  0.4134020209312439,\n",
       "  0.0),\n",
       " (\"Recevez des médicaments bon marché sans ordonnance. Commandez dès aujourd'hui !\",\n",
       "  0.339819073677063,\n",
       "  0.0),\n",
       " (\"Je le demande quesqui est le plus bizarre, un jeune homme qui vas à l'école avec son chien d'aveugle, ou le prof avec des cheveux\",\n",
       "  0.6707816123962402,\n",
       "  0.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=[(df_neg_ol.message.tolist()[i], clf(X_train[df_neg_ol.index[i]].unsqueeze(0)).item(), df_neg_ol.Label.tolist()[i]) for i in selected_indices_neg_ol]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3af1fa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4651058465242386"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1=[i[1] for i in d if i[0] in data1['message']]\n",
    "sum(d1)/len(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "def find_best_matches(embeddings_list1, embeddings_list2):\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings_list1, embeddings_list2)\n",
    "\n",
    "    # Use the Hungarian algorithm to find the optimal assignment\n",
    "    row_indices, col_indices = linear_sum_assignment(-similarity_matrix)\n",
    "\n",
    "    # Extract the pairs of best matches and their corresponding similarity scores\n",
    "    best_matches = [(row_indices[i], col_indices[i]) for i in range(len(row_indices))]\n",
    "    similarity_scores = [-similarity_matrix[row][col] for row, col in best_matches]\n",
    "\n",
    "    return best_matches, similarity_scores\n",
    "\n",
    "def average_mbm_similarity(embeddings_list1, embeddings_list2):\n",
    "    # Find the best matches and their similarity scores\n",
    "    best_matches, similarity_scores = find_best_matches(embeddings_list1, embeddings_list2)\n",
    "\n",
    "    # Compute the average cosine similarity using the similarity scores\n",
    "    avg_similarity = np.mean(np.abs(similarity_scores))\n",
    "    \n",
    "    return avg_similarity\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "def get_explanation(i):\n",
    "    \n",
    "        df = df_construct(i, train_idxs)\n",
    "        df_pos_sl, df_pos_ol = input_data(df, i, sett='positive')\n",
    "        df_neg_ol, df_neg_sl = input_data(df, i, sett='negative')\n",
    "        selected_indices_pos_sl = greedy_subset_selection(df_pos_sl, N=5, sett='positive', label='same')\n",
    "        selected_indices_pos_ol = greedy_subset_selection(df_pos_ol, N=5, sett='positive', label='opposite')\n",
    "        selected_indices_neg_sl = greedy_subset_selection(df_neg_sl, N=5, sett='negative', label='same')\n",
    "        selected_indices_neg_ol = greedy_subset_selection(df_neg_ol, N=5, sett='negative', label='opposite')\n",
    "        a=[df_pos_sl.Influence.index[k] for k in selected_indices_pos_sl]\n",
    "        b=[df_neg_ol.Influence.index[k] for k in selected_indices_neg_ol]\n",
    "        c=[df_neg_sl.Influence.index[k] for k in selected_indices_neg_sl]\n",
    "        d=[df_pos_ol.Influence.index[k] for k in selected_indices_pos_ol]\n",
    "        return a+b\n",
    "\n",
    "def aide_eval(test_idx):\n",
    "    simlist=np.array([cosine_similarity(X_test[test_idx].numpy().reshape(1,-1), X_test[i].numpy().reshape(1, -1)).item() for i in range(len(X_test))])\n",
    "    mostsim=simlist.argsort()[-5:]\n",
    "    leastsim=simlist.argsort()[:5]\n",
    "    concatenated_array = np.concatenate((mostsim, leastsim)).tolist()\n",
    "    cosine_sim=simlist[[concatenated_array]]\n",
    "    set1=get_explanation(test_idx)\n",
    "#     mean_ex_similarity=[]\n",
    "#     jaccard_sim=[]\n",
    "    mbm_sim=[]\n",
    "    fuzzy_jac=[]\n",
    "    for i in tqdm(concatenated_array):\n",
    "        set2=get_explanation(i)\n",
    "#         jaccard_sim.append(jaccard_similarity(set(set1), set(set2)))\n",
    "#         similarity_matrix = cosine_similarity(X_train[[set1]], X_train[[set2]])\n",
    "#         mean_ex_similarity.append(np.mean(similarity_matrix))\n",
    "        mbm=average_mbm_similarity(X_train[[set1]], X_train[[set2]])\n",
    "        mbm_sim.append(mbm)\n",
    "        fuzzy_jac.append(mbm/(len(set2)/5 - mbm))\n",
    "    return cosine_sim.flatten().tolist(), mbm_sim, fuzzy_jac\n",
    "\n",
    "import random\n",
    "sample_idx = random.sample(range(0, X_test.shape[0]), 10)\n",
    "cosine_total=[]\n",
    "mbm_total=[]\n",
    "fuzzy_total=[]\n",
    "# jaccard_total=[]\n",
    "for i in tqdm(sample_idx):\n",
    "    cosine_sim, mbm_sim, fuzzy_jac = aide_eval(i)\n",
    "    cosine_total.append(cosine_sim)\n",
    "    mbm_total.append(mbm_sim)\n",
    "    fuzzy_total.append(fuzzy_jac)\n",
    "#     mean_total.append(mean_ex_similarity)\n",
    "#     jaccard_total.append(jaccard_sim)\n",
    "    \n",
    "# plt.scatter(cosine_total, mbm_total, alpha=0.5)\n",
    "# plt.title(\"AIDE Faithfulness\")\n",
    "# plt.xlabel('Cosine Similarity of Emails')\n",
    "# plt.ylabel('Maximum Bipartite Matching Similarity of Explanations')\n",
    "# plt.legend(fontsize=7)\n",
    "\n",
    "plt.scatter(cosine_total, mbm_total, alpha=0.5)\n",
    "plt.title(\"AIDE Faithfulness\")\n",
    "plt.xlabel('Cosine Similarity of Emails')\n",
    "plt.ylabel('Fuzzy Jaccard Similarity of Explanations')\n",
    "plt.legend(fontsize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cosine_total, fuzzy_total, alpha=0.5)\n",
    "plt.title(\"AIDE Faithfulness\")\n",
    "plt.xlabel('Cosine Similarity of Emails')\n",
    "plt.ylabel('Fuzzy Jaccard Similarity of Explanations')\n",
    "plt.legend(fontsize=7)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f948149d",
   "metadata": {},
   "source": [
    "My slave! I want you to take 2 or 3 pictures of yourself today in bright light on your cell phone! Bright light!',\n",
    "  0.0\n",
    " \n",
    " \"Do you realize that in about 40 years, we'll have thousands of old ladies running around with tattoos?\",\n",
    "  1.0\n",
    "  \n",
    "  'Its a valentine game. . . Send dis msg to all ur friends. .. If 5 answers r d same then someone really loves u. Ques- which colour suits me the best?rply me'  0\n",
    "  \n",
    "  \"Do you ever notice that when you're driving, anyone going slower than you is an idiot and everyone driving faster than you is a maniac?\",\n",
    "  1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8469b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Combine the training and test tensors\n",
    "X = torch.cat((X_train, X_test), dim=0)\n",
    "y = torch.cat((y_train, y_test), dim=0)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "X = X.numpy()\n",
    "y = y.numpy()\n",
    "\n",
    "# Initialize the t-SNE algorithm\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit and transform the data to 2D\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the data in 2D with different colors and alpha values for each class\n",
    "plt.scatter(X_2d[y==0, 0], X_2d[y==0, 1], marker='o', c='#7FFFD4',  label='Ham', alpha=0.7)\n",
    "plt.scatter(X_2d[y==1, 0], X_2d[y==1, 1], marker='o', c='#FFE5B4',  label='Spam', alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(X_2d[len(y_train)+test_idx][0], X_2d[len(y_train)+test_idx][1], marker='.', color='red')\n",
    "\n",
    "plt.scatter([X_2d[df_pos_sl.index[i]][0] for i in selected_indices_pos_sl],\n",
    "        [X_2d[df_pos_sl.index[i]][1] for i in selected_indices_pos_sl], marker='+', color='black', label='Support by Relevance')\n",
    "\n",
    "# plt.scatter([X_2d[df_pos_ol.index[i]][0] for i in selected_indices_pos_ol],\n",
    "#         [X_2d[df_pos_ol.index[i]][1] for i in selected_indices_pos_ol], marker='x', color='orange', label='Support by Contrast')\n",
    "\n",
    "# plt.scatter([X_2d[df_neg_sl.index[i]][0] for i in selected_indices_neg_sl],\n",
    "#         [X_2d[df_neg_sl.index[i]][1] for i in selected_indices_neg_sl], marker='+', color='black', label='Oppose by Contrast')\n",
    "\n",
    "plt.scatter([X_2d[df_neg_ol.index[i]][0] for i in selected_indices_neg_ol],\n",
    "        [X_2d[df_neg_ol.index[i]][1] for i in selected_indices_neg_ol], marker='x', color='blue', label='Oppose by Relevance')\n",
    "\n",
    "# plt.scatter([X_2d[df_pos_sl.index[i]][0] for i in range(3)], [X_2d[df_pos_sl.index[i]][1] for i in range(3)],\n",
    "#             marker='*', color='red', label='Influential Instances')\n",
    "# plt.scatter([X_2d[df_pos_sl.sort_values('Similarity', ascending=False).index[i]][0] for i in range(3)],\n",
    "#             [X_2d[df_pos_sl.sort_values('Similarity', ascending=False).index[i]][1] for i in range(3)],\n",
    "#             marker='+', color='green', label='Similar Instances')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(f'tsne_aide{test_idx}.pdf', format='pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
